{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25be397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.patches import Patch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cb50a",
   "metadata": {},
   "source": [
    "#### Code to get gridded erosion data from cloud server and cluster into parallel data cubes of erosion metrics and cluster IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4c0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from reefbreak by location\n",
    "def find_csv_files(location, erosion=True, res_cm=10, cleaned=False):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of paths to all *_grid_{res_cm}cm.csv\n",
    "    in /‚Ä¶/results/<location>/{erosion,deposition}/<date>/.\n",
    "    \"\"\"\n",
    "    base = \"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results\"\n",
    "    mode = \"erosion\" if erosion else \"deposition\"\n",
    "    root = os.path.join(base, location, mode)\n",
    "    \n",
    "    date_re = re.compile(r\"(\\d{8})\")\n",
    "    out = []\n",
    "    for date in sorted(os.listdir(root)):\n",
    "        ddir = os.path.join(root, date)\n",
    "        if not os.path.isdir(ddir):\n",
    "            continue\n",
    "        for fn in os.listdir(ddir):\n",
    "            if not cleaned:\n",
    "                if fn.endswith(f\"_grid_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "            else:\n",
    "                if fn.endswith(f\"_grid_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "    # sort by date prefix\n",
    "    out.sort(key=lambda x: x[0])\n",
    "    return [path for (_, path) in out]\n",
    "\n",
    "\n",
    "def find_cluster_csv_files(location, erosion=True, res_cm=10, cleaned=False):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of paths to all *_clusters_{res_cm}cm.csv files\n",
    "    in /‚Ä¶/results/<location>/{erosion,deposition}/<date>/.\n",
    "    \"\"\"\n",
    "    base = \"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results\"\n",
    "    mode = \"erosion\" if erosion else \"deposition\"\n",
    "    root = os.path.join(base, location, mode)\n",
    "    \n",
    "    date_re = re.compile(r\"(\\d{8})\")\n",
    "    out = []\n",
    "    for date in sorted(os.listdir(root)):\n",
    "        ddir = os.path.join(root, date)\n",
    "        if not os.path.isdir(ddir):\n",
    "            continue\n",
    "        for fn in os.listdir(ddir):\n",
    "            if not cleaned:\n",
    "                # Look for cluster files instead of grid files\n",
    "                if fn.endswith(f\"_clusters_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "            else:\n",
    "                if fn.endswith(f\"_clusters_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "    # sort by date prefix\n",
    "    out.sort(key=lambda x: x[0])\n",
    "    return [path for (_, path) in out]\n",
    "\n",
    "# Load both grid and cluster data for a location\n",
    "def load_grid_and_cluster_cubes(location, erosion=True, res_cm=10, cleaned=False):\n",
    "    \"\"\"\n",
    "    Load both erosion/deposition grid data and cluster data for a location.\n",
    "    Returns grid_cube, grid_files, cluster_cube, cluster_files\n",
    "    \"\"\"\n",
    "    # Load grid data\n",
    "    grid_files = find_csv_files(location, erosion=erosion, res_cm=res_cm, cleaned=cleaned)\n",
    "    grid_cube, valid_grid_files = load_csv_to_numpy(grid_files)\n",
    "    \n",
    "    # Load cluster data\n",
    "    cluster_files = find_cluster_csv_files(location, erosion=erosion, res_cm=res_cm, cleaned=cleaned)\n",
    "    cluster_cube, valid_cluster_files = load_csv_to_numpy(cluster_files)\n",
    "    \n",
    "    print(f\"Grid cube shape: {grid_cube.shape}\")\n",
    "    print(f\"Cluster cube shape: {cluster_cube.shape}\")\n",
    "    \n",
    "    return grid_cube, valid_grid_files, cluster_cube, valid_cluster_files\n",
    "\n",
    "# turn csv grids into numpy arrays\n",
    "def load_csv_to_numpy(file_list):\n",
    "    \"\"\"\n",
    "    Loads CSV grids into a 3D NumPy array, with error checking for shape mismatches.\n",
    "    Mismatched grids are omitted from the final cube.\n",
    "    \"\"\"\n",
    "    grids = []\n",
    "    shapes = []\n",
    "    valid_files = []\n",
    "    \n",
    "    for fp in file_list:\n",
    "        df = pd.read_csv(fp, index_col=0)\n",
    "        grids.append(df.values)\n",
    "        shapes.append((fp, df.shape))\n",
    "        valid_files.append(fp)\n",
    "\n",
    "    if not grids:\n",
    "        raise ValueError(\"No valid CSV files found.\")\n",
    "    \n",
    "    # Get reference shape from first file\n",
    "    ref_shape = shapes[0][1]\n",
    "    \n",
    "    # Filter out mismatched grids\n",
    "    valid_grids = []\n",
    "    valid_files_final = []\n",
    "    mismatches = []\n",
    "    \n",
    "    for i, (fp, shape) in enumerate(shapes):\n",
    "        if shape == ref_shape:\n",
    "            valid_grids.append(grids[i])\n",
    "            valid_files_final.append(fp)\n",
    "        else:\n",
    "            mismatches.append((fp, shape))\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"\\n‚ö†Ô∏è Omitting {len(mismatches)} files with mismatched grid shapes:\")\n",
    "        for fp, shape in mismatches:\n",
    "            print(f\"  {os.path.basename(fp)} ‚Äî shape = {shape}, expected = {ref_shape}\")\n",
    "        print(f\"\\n‚úÖ Using {len(valid_grids)} files with consistent shape {ref_shape}\")\n",
    "    \n",
    "    if not valid_grids:\n",
    "        raise ValueError(\"No files have consistent grid shapes.\")\n",
    "    \n",
    "    return np.stack(valid_grids, axis=0), valid_files_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db555bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all locations to process\n",
    "locations = [\"Delmar\", \"Torrey\", \"Solana\", \"Encinitas\", \"SanElijo\", \"Blacks\"]\n",
    "\n",
    "def process_location(location):\n",
    "    \"\"\"Process a single location: load data, create cubes, and save files\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {location}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        # Create data_cubes directory if it doesn't exist\n",
    "        output_dir = f\"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/{location}/data_cubes\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load both grid and cluster data for erosion\n",
    "        grid_cube, grid_files, cluster_cube, cluster_files = load_grid_and_cluster_cubes(\n",
    "            location, erosion=True, res_cm=10, cleaned=False\n",
    "        )\n",
    "        \n",
    "        # Save both cubes\n",
    "        grid_path = os.path.join(output_dir, \"cube_ero_10cm_filled.npz\")\n",
    "        cluster_path = os.path.join(output_dir, \"cube_clusters_ero_10cm_filled.npz\")\n",
    "        \n",
    "        np.savez_compressed(grid_path, data=grid_cube)\n",
    "        np.savez_compressed(cluster_path, data=cluster_cube)\n",
    "        \n",
    "        print(f\"‚úÖ Saved grid cube: {grid_path}\")\n",
    "        print(f\"‚úÖ Saved cluster cube: {cluster_path}\")\n",
    "        \n",
    "        # Save file lists\n",
    "        grid_files_path = os.path.join(output_dir, \"files_ero.json\")\n",
    "        cluster_files_path = os.path.join(output_dir, \"files_clusters_ero.json\")\n",
    "        \n",
    "        with open(grid_files_path, \"w\") as f:\n",
    "            json.dump(grid_files, f, indent=2)\n",
    "        \n",
    "        with open(cluster_files_path, \"w\") as f:\n",
    "            json.dump(cluster_files, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Saved grid file list: {grid_files_path}\")\n",
    "        print(f\"‚úÖ Saved cluster file list: {cluster_files_path}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüìä {location} Summary:\")\n",
    "        print(f\"   Grid cube shape: {grid_cube.shape}\")\n",
    "        print(f\"   Cluster cube shape: {cluster_cube.shape}\")\n",
    "        print(f\"   Number of time steps: {len(grid_files)}\")\n",
    "        print(f\"   Grid file size: {grid_cube.nbytes / (1024**3):.2f} GB\")\n",
    "        print(f\"   Cluster file size: {cluster_cube.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {location}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Process all locations\n",
    "successful_locations = []\n",
    "failed_locations = []\n",
    "\n",
    "print(\"Starting batch processing of all locations...\")\n",
    "print(f\"Locations to process: {', '.join(locations)}\")\n",
    "\n",
    "for location in locations:\n",
    "    success = process_location(location)\n",
    "    \n",
    "    if success:\n",
    "        successful_locations.append(location)\n",
    "    else:\n",
    "        failed_locations.append(location)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BATCH PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully processed ({len(successful_locations)}/{len(locations)}):\")\n",
    "for loc in successful_locations:\n",
    "    print(f\"   - {loc}\")\n",
    "\n",
    "if failed_locations:\n",
    "    print(f\"\\n‚ùå Failed to process ({len(failed_locations)}/{len(locations)}):\")\n",
    "    for loc in failed_locations:\n",
    "        print(f\"   - {loc}\")\n",
    "else:\n",
    "    print(f\"\\nüéâ All {len(locations)} locations processed successfully!\")\n",
    "\n",
    "print(f\"\\nData cubes saved to:\")\n",
    "for loc in successful_locations:\n",
    "    print(f\"   {loc}: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/{loc}/data_cubes/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coastal-cliff-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
