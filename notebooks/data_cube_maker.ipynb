{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25be397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.patches import Patch\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cb50a",
   "metadata": {},
   "source": [
    "#### Code to get gridded erosion data from cloud server and cluster into parallel data cubes of erosion metrics and cluster IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4c0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from reefbreak by location\n",
    "def find_csv_files(location, erosion=True, res_cm=10, cleaned=False):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of paths to all *_grid_{res_cm}cm.csv\n",
    "    in /‚Ä¶/results/<location>/{erosion,deposition}/<date>/.\n",
    "    \"\"\"\n",
    "    base = \"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results\"\n",
    "    mode = \"erosion\" if erosion else \"deposition\"\n",
    "    root = os.path.join(base, location, mode)\n",
    "    \n",
    "    date_re = re.compile(r\"(\\d{8})\")\n",
    "    out = []\n",
    "    for date in sorted(os.listdir(root)):\n",
    "        ddir = os.path.join(root, date)\n",
    "        if not os.path.isdir(ddir):\n",
    "            continue\n",
    "        for fn in os.listdir(ddir):\n",
    "            if not cleaned:\n",
    "                if fn.endswith(f\"_grid_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "            else:\n",
    "                if fn.endswith(f\"_grid_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "    # sort by date prefix\n",
    "    out.sort(key=lambda x: x[0])\n",
    "    return [path for (_, path) in out]\n",
    "\n",
    "\n",
    "def find_cluster_csv_files(location, erosion=True, res_cm=10, cleaned=False):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of paths to all *_clusters_{res_cm}cm.csv files\n",
    "    in /‚Ä¶/results/<location>/{erosion,deposition}/<date>/.\n",
    "    \"\"\"\n",
    "    base = \"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results\"\n",
    "    mode = \"erosion\" if erosion else \"deposition\"\n",
    "    root = os.path.join(base, location, mode)\n",
    "    \n",
    "    date_re = re.compile(r\"(\\d{8})\")\n",
    "    out = []\n",
    "    for date in sorted(os.listdir(root)):\n",
    "        ddir = os.path.join(root, date)\n",
    "        if not os.path.isdir(ddir):\n",
    "            continue\n",
    "        for fn in os.listdir(ddir):\n",
    "            if not cleaned:\n",
    "                # Look for cluster files instead of grid files\n",
    "                if fn.endswith(f\"_clusters_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "            else:\n",
    "                if fn.endswith(f\"_clusters_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "    # sort by date prefix\n",
    "    out.sort(key=lambda x: x[0])\n",
    "    return [path for (_, path) in out]\n",
    "\n",
    "# Load both grid and cluster data for a location\n",
    "def load_grid_and_cluster_cubes(location, erosion=True, res_cm=10, cleaned=False):\n",
    "    \"\"\"\n",
    "    Load both erosion/deposition grid data and cluster data for a location.\n",
    "    Returns grid_cube, grid_files, cluster_cube, cluster_files\n",
    "    \"\"\"\n",
    "    # Load grid data\n",
    "    grid_files = find_csv_files(location, erosion=erosion, res_cm=res_cm, cleaned=cleaned)\n",
    "    grid_cube, valid_grid_files = load_csv_to_numpy(grid_files)\n",
    "    \n",
    "    # Load cluster data\n",
    "    cluster_files = find_cluster_csv_files(location, erosion=erosion, res_cm=res_cm, cleaned=cleaned)\n",
    "    cluster_cube, valid_cluster_files = load_csv_to_numpy(cluster_files)\n",
    "    \n",
    "    print(f\"Grid cube shape: {grid_cube.shape}\")\n",
    "    print(f\"Cluster cube shape: {cluster_cube.shape}\")\n",
    "    \n",
    "    return grid_cube, valid_grid_files, cluster_cube, valid_cluster_files\n",
    "\n",
    "# turn csv grids into numpy arrays\n",
    "def load_csv_to_numpy(file_list):\n",
    "    \"\"\"\n",
    "    Loads CSV grids into a 3D NumPy array, with error checking for shape mismatches.\n",
    "    Mismatched grids are omitted from the final cube.\n",
    "    \"\"\"\n",
    "    grids = []\n",
    "    shapes = []\n",
    "    valid_files = []\n",
    "    \n",
    "    for fp in file_list:\n",
    "        df = pd.read_csv(fp, index_col=0)\n",
    "        grids.append(df.values)\n",
    "        shapes.append((fp, df.shape))\n",
    "        valid_files.append(fp)\n",
    "\n",
    "    if not grids:\n",
    "        raise ValueError(\"No valid CSV files found.\")\n",
    "    \n",
    "    # Get reference shape from first file\n",
    "    ref_shape = shapes[0][1]\n",
    "    \n",
    "    # Filter out mismatched grids\n",
    "    valid_grids = []\n",
    "    valid_files_final = []\n",
    "    mismatches = []\n",
    "    \n",
    "    for i, (fp, shape) in enumerate(shapes):\n",
    "        if shape == ref_shape:\n",
    "            valid_grids.append(grids[i])\n",
    "            valid_files_final.append(fp)\n",
    "        else:\n",
    "            mismatches.append((fp, shape))\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"\\n‚ö†Ô∏è Omitting {len(mismatches)} files with mismatched grid shapes:\")\n",
    "        for fp, shape in mismatches:\n",
    "            print(f\"  {os.path.basename(fp)} ‚Äî shape = {shape}, expected = {ref_shape}\")\n",
    "        print(f\"\\n‚úÖ Using {len(valid_grids)} files with consistent shape {ref_shape}\")\n",
    "    \n",
    "    if not valid_grids:\n",
    "        raise ValueError(\"No files have consistent grid shapes.\")\n",
    "    \n",
    "    return np.stack(valid_grids, axis=0), valid_files_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db555bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing of all locations...\n",
      "Locations to process: Torrey, Solana, Encinitas, SanElijo, Blacks\n",
      "\n",
      "==================================================\n",
      "Processing Torrey...\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è Omitting 3 files with mismatched grid shapes:\n",
      "  20220518_to_20220518_ero_grid_10cm_filled.csv ‚Äî shape = (12888, 1), expected = (12888, 750)\n",
      "  20230120_to_20230120_ero_grid_10cm_filled.csv ‚Äî shape = (12888, 1), expected = (12888, 750)\n",
      "  20241126_to_20241126_ero_grid_10cm_filled.csv ‚Äî shape = (12888, 1), expected = (12888, 750)\n",
      "\n",
      "‚úÖ Using 301 files with consistent shape (12888, 750)\n",
      "\n",
      "‚ö†Ô∏è Omitting 3 files with mismatched grid shapes:\n",
      "  20220518_to_20220518_ero_clusters_10cm_filled.csv ‚Äî shape = (12888, 1), expected = (12888, 750)\n",
      "  20230120_to_20230120_ero_clusters_10cm_filled.csv ‚Äî shape = (12888, 1), expected = (12888, 750)\n",
      "  20241126_to_20241126_ero_clusters_10cm_filled.csv ‚Äî shape = (12888, 1), expected = (12888, 750)\n",
      "\n",
      "‚úÖ Using 301 files with consistent shape (12888, 750)\n",
      "Grid cube shape: (301, 12888, 750)\n",
      "Cluster cube shape: (301, 12888, 750)\n",
      "‚úÖ Saved grid cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/cube_ero_10cm_filled.npz\n",
      "‚úÖ Saved cluster cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/cube_clusters_ero_10cm_filled.npz\n",
      "‚úÖ Saved grid file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/files_ero.json\n",
      "‚úÖ Saved cluster file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/files_clusters_ero.json\n",
      "\n",
      "üìä Torrey Summary:\n",
      "   Grid cube shape: (301, 12888, 750)\n",
      "   Cluster cube shape: (301, 12888, 750)\n",
      "   Number of time steps: 301\n",
      "   Grid file size: 21.68 GB\n",
      "   Cluster file size: 21.68 GB\n",
      "\n",
      "==================================================\n",
      "Processing Solana...\n",
      "==================================================\n",
      "Grid cube shape: (117, 28845, 500)\n",
      "Cluster cube shape: (117, 28845, 500)\n",
      "‚úÖ Saved grid cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/cube_ero_10cm_filled.npz\n",
      "‚úÖ Saved cluster cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/cube_clusters_ero_10cm_filled.npz\n",
      "‚úÖ Saved grid file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/files_ero.json\n",
      "‚úÖ Saved cluster file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/files_clusters_ero.json\n",
      "\n",
      "üìä Solana Summary:\n",
      "   Grid cube shape: (117, 28845, 500)\n",
      "   Cluster cube shape: (117, 28845, 500)\n",
      "   Number of time steps: 117\n",
      "   Grid file size: 12.57 GB\n",
      "   Cluster file size: 12.57 GB\n",
      "\n",
      "==================================================\n",
      "Processing Encinitas...\n",
      "==================================================\n",
      "Grid cube shape: (104, 55824, 500)\n",
      "Cluster cube shape: (104, 55824, 500)\n",
      "‚úÖ Saved grid cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/cube_ero_10cm_filled.npz\n",
      "‚úÖ Saved cluster cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/cube_clusters_ero_10cm_filled.npz\n",
      "‚úÖ Saved grid file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/files_ero.json\n",
      "‚úÖ Saved cluster file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/files_clusters_ero.json\n",
      "\n",
      "üìä Encinitas Summary:\n",
      "   Grid cube shape: (104, 55824, 500)\n",
      "   Cluster cube shape: (104, 55824, 500)\n",
      "   Number of time steps: 104\n",
      "   Grid file size: 21.63 GB\n",
      "   Cluster file size: 21.63 GB\n",
      "\n",
      "==================================================\n",
      "Processing SanElijo...\n",
      "==================================================\n",
      "Grid cube shape: (115, 23825, 399)\n",
      "Cluster cube shape: (115, 23825, 399)\n",
      "‚úÖ Saved grid cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/cube_ero_10cm_filled.npz\n",
      "‚úÖ Saved cluster cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/cube_clusters_ero_10cm_filled.npz\n",
      "‚úÖ Saved grid file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/files_ero.json\n",
      "‚úÖ Saved cluster file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/files_clusters_ero.json\n",
      "\n",
      "üìä SanElijo Summary:\n",
      "   Grid cube shape: (115, 23825, 399)\n",
      "   Cluster cube shape: (115, 23825, 399)\n",
      "   Number of time steps: 115\n",
      "   Grid file size: 8.15 GB\n",
      "   Cluster file size: 8.15 GB\n",
      "\n",
      "==================================================\n",
      "Processing Blacks...\n",
      "==================================================\n",
      "‚ùå Error processing Blacks: No valid CSV files found.\n",
      "\n",
      "============================================================\n",
      "BATCH PROCESSING COMPLETE\n",
      "============================================================\n",
      "\n",
      "‚úÖ Successfully processed (4/5):\n",
      "   - Torrey\n",
      "   - Solana\n",
      "   - Encinitas\n",
      "   - SanElijo\n",
      "\n",
      "‚ùå Failed to process (1/5):\n",
      "   - Blacks\n",
      "\n",
      "Data cubes saved to:\n",
      "   Torrey: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/\n",
      "   Solana: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/\n",
      "   Encinitas: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/\n",
      "   SanElijo: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/\n"
     ]
    }
   ],
   "source": [
    "# List of all locations to process\n",
    "locations = [\"Torrey\", \"Solana\", \"Encinitas\", \"SanElijo\", \"Blacks\"] # Delmar\", \n",
    "\n",
    "def process_location(location):\n",
    "    \"\"\"Process a single location: load data, create cubes, and save files\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {location}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        # Create data_cubes directory if it doesn't exist\n",
    "        output_dir = f\"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/{location}/data_cubes\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load both grid and cluster data for erosion\n",
    "        grid_cube, grid_files, cluster_cube, cluster_files = load_grid_and_cluster_cubes(\n",
    "            location, erosion=True, res_cm=10, cleaned=False\n",
    "        )\n",
    "        \n",
    "        # Save both cubes\n",
    "        grid_path = os.path.join(output_dir, \"cube_ero_10cm_filled.npz\")\n",
    "        cluster_path = os.path.join(output_dir, \"cube_clusters_ero_10cm_filled.npz\")\n",
    "        \n",
    "        np.savez_compressed(grid_path, data=grid_cube)\n",
    "        np.savez_compressed(cluster_path, data=cluster_cube)\n",
    "        \n",
    "        print(f\"‚úÖ Saved grid cube: {grid_path}\")\n",
    "        print(f\"‚úÖ Saved cluster cube: {cluster_path}\")\n",
    "        \n",
    "        # Save file lists\n",
    "        grid_files_path = os.path.join(output_dir, \"files_ero.json\")\n",
    "        cluster_files_path = os.path.join(output_dir, \"files_clusters_ero.json\")\n",
    "        \n",
    "        with open(grid_files_path, \"w\") as f:\n",
    "            json.dump(grid_files, f, indent=2)\n",
    "        \n",
    "        with open(cluster_files_path, \"w\") as f:\n",
    "            json.dump(cluster_files, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Saved grid file list: {grid_files_path}\")\n",
    "        print(f\"‚úÖ Saved cluster file list: {cluster_files_path}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüìä {location} Summary:\")\n",
    "        print(f\"   Grid cube shape: {grid_cube.shape}\")\n",
    "        print(f\"   Cluster cube shape: {cluster_cube.shape}\")\n",
    "        print(f\"   Number of time steps: {len(grid_files)}\")\n",
    "        print(f\"   Grid file size: {grid_cube.nbytes / (1024**3):.2f} GB\")\n",
    "        print(f\"   Cluster file size: {cluster_cube.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {location}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Process all locations\n",
    "successful_locations = []\n",
    "failed_locations = []\n",
    "\n",
    "print(\"Starting batch processing of all locations...\")\n",
    "print(f\"Locations to process: {', '.join(locations)}\")\n",
    "\n",
    "for location in locations:\n",
    "    success = process_location(location)\n",
    "    \n",
    "    if success:\n",
    "        successful_locations.append(location)\n",
    "    else:\n",
    "        failed_locations.append(location)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BATCH PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully processed ({len(successful_locations)}/{len(locations)}):\")\n",
    "for loc in successful_locations:\n",
    "    print(f\"   - {loc}\")\n",
    "\n",
    "if failed_locations:\n",
    "    print(f\"\\n‚ùå Failed to process ({len(failed_locations)}/{len(locations)}):\")\n",
    "    for loc in failed_locations:\n",
    "        print(f\"   - {loc}\")\n",
    "else:\n",
    "    print(f\"\\nüéâ All {len(locations)} locations processed successfully!\")\n",
    "\n",
    "print(f\"\\nData cubes saved to:\")\n",
    "for loc in successful_locations:\n",
    "    print(f\"   {loc}: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/{loc}/data_cubes/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca972ec",
   "metadata": {},
   "source": [
    "##### Code to split erosion cubes into small small and sig erosion events (10 cubic meter threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5503ff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EROSION SEPARATION BY CLUSTER VOLUME (PER TIMESTEP)\n",
      "============================================================\n",
      "\n",
      "Configuration:\n",
      "  Volume threshold: 10.0 m¬≥\n",
      "  Spatial resolution: 10 cm\n",
      "  Locations to process: 5\n",
      "  Processing: Each timestep independently (cluster IDs reset each timestep)\n",
      "\n",
      "============================================================\n",
      "Processing Delmar - Volume Threshold: 10.0 m¬≥\n",
      "============================================================\n",
      "Loading data cubes...\n",
      "  Erosion cube shape: (321, 22850, 300)\n",
      "  Cluster cube shape: (321, 22850, 300)\n",
      "  Erosion value range: 0.00 to 18.07 cm\n",
      "  Cluster ID range: 1 to 1715\n",
      "Processing 321 timesteps...\n",
      "  Processed timestep 10/321 - Sig: 2, Small: 21\n",
      "  Processed timestep 20/321 - Sig: 3, Small: 59\n",
      "  Processed timestep 30/321 - Sig: 0, Small: 10\n",
      "  Processed timestep 40/321 - Sig: 0, Small: 9\n",
      "  Processed timestep 50/321 - Sig: 0, Small: 13\n",
      "  Processed timestep 60/321 - Sig: 0, Small: 8\n",
      "  Processed timestep 70/321 - Sig: 1, Small: 108\n",
      "  Processed timestep 80/321 - Sig: 9, Small: 234\n",
      "  Processed timestep 90/321 - Sig: 0, Small: 36\n",
      "  Processed timestep 100/321 - Sig: 0, Small: 13\n",
      "  Processed timestep 110/321 - Sig: 0, Small: 3\n",
      "  Processed timestep 120/321 - Sig: 1, Small: 18\n",
      "  Processed timestep 130/321 - Sig: 0, Small: 6\n",
      "  Processed timestep 140/321 - Sig: 0, Small: 11\n",
      "  Processed timestep 150/321 - Sig: 0, Small: 15\n",
      "  Processed timestep 160/321 - Sig: 1, Small: 22\n",
      "  Processed timestep 170/321 - Sig: 0, Small: 23\n",
      "  Processed timestep 180/321 - Sig: 0, Small: 77\n",
      "  Processed timestep 190/321 - Sig: 1, Small: 27\n",
      "  Processed timestep 200/321 - Sig: 1, Small: 71\n",
      "  Processed timestep 210/321 - Sig: 0, Small: 15\n",
      "  Processed timestep 220/321 - Sig: 0, Small: 2\n",
      "  Processed timestep 230/321 - Sig: 0, Small: 1\n",
      "  Processed timestep 240/321 - Sig: 0, Small: 34\n",
      "  Processed timestep 250/321 - Sig: 2, Small: 43\n",
      "  Processed timestep 260/321 - Sig: 0, Small: 2\n",
      "  Processed timestep 270/321 - Sig: 1, Small: 22\n",
      "  Processed timestep 280/321 - Sig: 0, Small: 1\n",
      "  Processed timestep 290/321 - Sig: 0, Small: 51\n",
      "  Processed timestep 300/321 - Sig: 0, Small: 5\n",
      "  Processed timestep 310/321 - Sig: 4, Small: 44\n",
      "  Processed timestep 320/321 - Sig: 0, Small: 9\n",
      "  Processed timestep 321/321 - Sig: 0, Small: 4\n",
      "\n",
      "Cluster classification summary:\n",
      "  Total clusters across all timesteps: 13510\n",
      "  Significant clusters (>10.0 m¬≥): 336 (2.5%)\n",
      "  Small clusters (‚â§10.0 m¬≥): 13174 (97.5%)\n",
      "  Average clusters per timestep: 42.1\n",
      "\n",
      "Saving separated cubes...\n",
      "‚úÖ Saved significant erosion: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Delmar/data_cubes/cube_ero_10cm_sig.npz\n",
      "‚úÖ Saved small erosion: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Delmar/data_cubes/cube_ero_10cm_small.npz\n",
      "‚úÖ Saved significant clusters: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Delmar/data_cubes/cube_clusters_10cm_sig.npz\n",
      "‚úÖ Saved small clusters: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Delmar/data_cubes/cube_clusters_10cm_small.npz\n",
      "‚úÖ Saved cluster volumes: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Delmar/data_cubes/cluster_volumes_by_timestep.json\n",
      "‚úÖ Saved statistics: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Delmar/data_cubes/separation_stats.json\n",
      "\n",
      "üìä Delmar Statistics:\n",
      "   Total timesteps: 321\n",
      "   Total clusters: 13510\n",
      "   Significant clusters: 336 (2.5%)\n",
      "   Small clusters: 13174 (97.5%)\n",
      "   Average clusters per timestep: 42.1\n",
      "   Average significant clusters per timestep: 1.0\n",
      "   Total significant volume: 15294.85 m¬≥\n",
      "   Total small volume: 10077.27 m¬≥\n",
      "   Volume fraction (significant): 60.3%\n",
      "\n",
      "   Top 10 largest individual clusters:\n",
      "      1. Timestep 182, Cluster 120: 1410.47 m¬≥\n",
      "      2. Timestep 60, Cluster 5: 933.00 m¬≥\n",
      "      3. Timestep 184, Cluster 25: 766.46 m¬≥\n",
      "      4. Timestep 183, Cluster 29: 467.50 m¬≥\n",
      "      5. Timestep 180, Cluster 16: 410.94 m¬≥\n",
      "      6. Timestep 189, Cluster 1: 378.05 m¬≥\n",
      "      7. Timestep 82, Cluster 197: 339.68 m¬≥\n",
      "      8. Timestep 246, Cluster 139: 300.92 m¬≥\n",
      "      9. Timestep 307, Cluster 44: 292.71 m¬≥\n",
      "      10. Timestep 307, Cluster 13: 280.87 m¬≥\n",
      "\n",
      "============================================================\n",
      "Processing Torrey - Volume Threshold: 10.0 m¬≥\n",
      "============================================================\n",
      "Loading data cubes...\n",
      "  Erosion cube shape: (301, 12888, 750)\n",
      "  Cluster cube shape: (301, 12888, 750)\n",
      "  Erosion value range: 0.00 to 18.96 cm\n",
      "  Cluster ID range: 1 to 5978\n",
      "Processing 301 timesteps...\n",
      "  Processed timestep 10/301 - Sig: 1, Small: 53\n",
      "  Processed timestep 20/301 - Sig: 1, Small: 60\n",
      "  Processed timestep 30/301 - Sig: 0, Small: 51\n",
      "  Processed timestep 40/301 - Sig: 0, Small: 31\n",
      "  Processed timestep 50/301 - Sig: 0, Small: 55\n",
      "  Processed timestep 60/301 - Sig: 4, Small: 57\n",
      "  Processed timestep 70/301 - Sig: 0, Small: 36\n",
      "  Processed timestep 80/301 - Sig: 4, Small: 93\n",
      "  Processed timestep 90/301 - Sig: 0, Small: 37\n",
      "  Processed timestep 100/301 - Sig: 1, Small: 57\n",
      "  Processed timestep 110/301 - Sig: 1, Small: 14\n",
      "  Processed timestep 120/301 - Sig: 0, Small: 78\n",
      "  Processed timestep 130/301 - Sig: 1, Small: 77\n",
      "  Processed timestep 140/301 - Sig: 0, Small: 205\n",
      "  Processed timestep 150/301 - Sig: 0, Small: 54\n",
      "  Processed timestep 160/301 - Sig: 0, Small: 3\n",
      "  Processed timestep 170/301 - Sig: 0, Small: 5\n",
      "  Processed timestep 180/301 - Sig: 0, Small: 72\n",
      "  Processed timestep 190/301 - Sig: 0, Small: 44\n",
      "  Processed timestep 200/301 - Sig: 0, Small: 10\n",
      "  Processed timestep 210/301 - Sig: 4, Small: 58\n",
      "  Processed timestep 220/301 - Sig: 0, Small: 7\n",
      "  Processed timestep 230/301 - Sig: 1, Small: 23\n",
      "  Processed timestep 240/301 - Sig: 9, Small: 227\n",
      "  Processed timestep 250/301 - Sig: 1, Small: 16\n",
      "  Processed timestep 260/301 - Sig: 0, Small: 21\n",
      "  Processed timestep 270/301 - Sig: 1, Small: 83\n",
      "  Processed timestep 280/301 - Sig: 0, Small: 11\n",
      "  Processed timestep 290/301 - Sig: 2, Small: 97\n",
      "  Processed timestep 300/301 - Sig: 0, Small: 79\n",
      "  Processed timestep 301/301 - Sig: 0, Small: 22\n",
      "\n",
      "Cluster classification summary:\n",
      "  Total clusters across all timesteps: 17983\n",
      "  Significant clusters (>10.0 m¬≥): 588 (3.3%)\n",
      "  Small clusters (‚â§10.0 m¬≥): 17395 (96.7%)\n",
      "  Average clusters per timestep: 59.7\n",
      "\n",
      "Saving separated cubes...\n",
      "‚úÖ Saved significant erosion: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/cube_ero_10cm_sig.npz\n",
      "‚úÖ Saved small erosion: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/cube_ero_10cm_small.npz\n",
      "‚úÖ Saved significant clusters: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/cube_clusters_10cm_sig.npz\n",
      "‚úÖ Saved small clusters: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/cube_clusters_10cm_small.npz\n",
      "‚úÖ Saved cluster volumes: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/cluster_volumes_by_timestep.json\n",
      "‚úÖ Saved statistics: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/separation_stats.json\n",
      "\n",
      "üìä Torrey Statistics:\n",
      "   Total timesteps: 301\n",
      "   Total clusters: 17983\n",
      "   Significant clusters: 588 (3.3%)\n",
      "   Small clusters: 17395 (96.7%)\n",
      "   Average clusters per timestep: 59.7\n",
      "   Average significant clusters per timestep: 2.0\n",
      "   Total significant volume: 16797.10 m¬≥\n",
      "   Total small volume: 12926.04 m¬≥\n",
      "   Volume fraction (significant): 56.5%\n",
      "\n",
      "   Top 10 largest individual clusters:\n",
      "      1. Timestep 120, Cluster 195: 1353.84 m¬≥\n",
      "      2. Timestep 171, Cluster 29: 557.69 m¬≥\n",
      "      3. Timestep 137, Cluster 407: 464.55 m¬≥\n",
      "      4. Timestep 37, Cluster 25: 369.83 m¬≥\n",
      "      5. Timestep 207, Cluster 41: 261.75 m¬≥\n",
      "      6. Timestep 200, Cluster 11: 207.40 m¬≥\n",
      "      7. Timestep 198, Cluster 27: 181.82 m¬≥\n",
      "      8. Timestep 258, Cluster 7: 166.99 m¬≥\n",
      "      9. Timestep 120, Cluster 41: 159.14 m¬≥\n",
      "      10. Timestep 12, Cluster 21: 154.67 m¬≥\n",
      "\n",
      "============================================================\n",
      "Processing Solana - Volume Threshold: 10.0 m¬≥\n",
      "============================================================\n",
      "Loading data cubes...\n",
      "  Erosion cube shape: (117, 28845, 500)\n",
      "  Cluster cube shape: (117, 28845, 500)\n",
      "  Erosion value range: 0.00 to 26.43 cm\n",
      "  Cluster ID range: 1 to 5358\n",
      "Processing 117 timesteps...\n",
      "  Processed timestep 10/117 - Sig: 7, Small: 58\n",
      "  Processed timestep 20/117 - Sig: 12, Small: 129\n",
      "  Processed timestep 30/117 - Sig: 3, Small: 99\n",
      "  Processed timestep 40/117 - Sig: 7, Small: 106\n",
      "  Processed timestep 50/117 - Sig: 0, Small: 3\n",
      "  Processed timestep 60/117 - Sig: 2, Small: 39\n",
      "  Processed timestep 70/117 - Sig: 4, Small: 71\n",
      "  Processed timestep 80/117 - Sig: 4, Small: 30\n",
      "  Processed timestep 90/117 - Sig: 4, Small: 90\n",
      "  Processed timestep 100/117 - Sig: 3, Small: 89\n",
      "  Processed timestep 110/117 - Sig: 1, Small: 37\n",
      "  Processed timestep 117/117 - Sig: 8, Small: 118\n",
      "\n",
      "Cluster classification summary:\n",
      "  Total clusters across all timesteps: 15184\n",
      "  Significant clusters (>10.0 m¬≥): 843 (5.6%)\n",
      "  Small clusters (‚â§10.0 m¬≥): 14341 (94.4%)\n",
      "  Average clusters per timestep: 129.8\n",
      "\n",
      "Saving separated cubes...\n",
      "‚úÖ Saved significant erosion: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/cube_ero_10cm_sig.npz\n",
      "‚úÖ Saved small erosion: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/cube_ero_10cm_small.npz\n",
      "‚úÖ Saved significant clusters: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/cube_clusters_10cm_sig.npz\n",
      "‚úÖ Saved small clusters: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/cube_clusters_10cm_small.npz\n",
      "‚úÖ Saved cluster volumes: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/cluster_volumes_by_timestep.json\n",
      "‚úÖ Saved statistics: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/separation_stats.json\n",
      "\n",
      "üìä Solana Statistics:\n",
      "   Total timesteps: 117\n",
      "   Total clusters: 15184\n",
      "   Significant clusters: 843 (5.6%)\n",
      "   Small clusters: 14341 (94.4%)\n",
      "   Average clusters per timestep: 129.8\n",
      "   Average significant clusters per timestep: 7.2\n",
      "   Total significant volume: 20261.03 m¬≥\n",
      "   Total small volume: 26181.75 m¬≥\n",
      "   Volume fraction (significant): 43.6%\n",
      "\n",
      "   Top 10 largest individual clusters:\n",
      "      1. Timestep 10, Cluster 32: 788.19 m¬≥\n",
      "      2. Timestep 10, Cluster 36: 595.42 m¬≥\n",
      "      3. Timestep 10, Cluster 34: 239.13 m¬≥\n",
      "      4. Timestep 60, Cluster 57: 189.64 m¬≥\n",
      "      5. Timestep 10, Cluster 30: 155.63 m¬≥\n",
      "      6. Timestep 22, Cluster 573: 138.83 m¬≥\n",
      "      7. Timestep 105, Cluster 47: 138.59 m¬≥\n",
      "      8. Timestep 101, Cluster 49: 135.55 m¬≥\n",
      "      9. Timestep 34, Cluster 105: 131.28 m¬≥\n",
      "      10. Timestep 1, Cluster 298: 111.66 m¬≥\n",
      "\n",
      "============================================================\n",
      "Processing Encinitas - Volume Threshold: 10.0 m¬≥\n",
      "============================================================\n",
      "Loading data cubes...\n",
      "  Erosion cube shape: (104, 55824, 500)\n",
      "  Cluster cube shape: (104, 55824, 500)\n",
      "  Erosion value range: 0.00 to 22.87 cm\n",
      "  Cluster ID range: 1 to 3987\n",
      "Processing 104 timesteps...\n",
      "  Processed timestep 10/104 - Sig: 22, Small: 419\n",
      "  Processed timestep 20/104 - Sig: 4, Small: 77\n",
      "  Processed timestep 30/104 - Sig: 0, Small: 43\n",
      "  Processed timestep 40/104 - Sig: 1, Small: 146\n",
      "  Processed timestep 50/104 - Sig: 2, Small: 138\n",
      "  Processed timestep 60/104 - Sig: 2, Small: 41\n",
      "  Processed timestep 70/104 - Sig: 5, Small: 246\n",
      "  Processed timestep 80/104 - Sig: 0, Small: 11\n",
      "  Processed timestep 90/104 - Sig: 1, Small: 231\n",
      "  Processed timestep 100/104 - Sig: 0, Small: 18\n",
      "  Processed timestep 104/104 - Sig: 1, Small: 102\n",
      "\n",
      "Cluster classification summary:\n",
      "  Total clusters across all timesteps: 19211\n",
      "  Significant clusters (>10.0 m¬≥): 704 (3.7%)\n",
      "  Small clusters (‚â§10.0 m¬≥): 18507 (96.3%)\n",
      "  Average clusters per timestep: 184.7\n",
      "\n",
      "Saving separated cubes...\n",
      "‚úÖ Saved significant erosion: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/cube_ero_10cm_sig.npz\n",
      "‚úÖ Saved small erosion: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/cube_ero_10cm_small.npz\n",
      "‚úÖ Saved significant clusters: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/cube_clusters_10cm_sig.npz\n",
      "‚úÖ Saved small clusters: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/cube_clusters_10cm_small.npz\n",
      "‚úÖ Saved cluster volumes: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/cluster_volumes_by_timestep.json\n",
      "‚úÖ Saved statistics: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/separation_stats.json\n",
      "\n",
      "üìä Encinitas Statistics:\n",
      "   Total timesteps: 104\n",
      "   Total clusters: 19211\n",
      "   Significant clusters: 704 (3.7%)\n",
      "   Small clusters: 18507 (96.3%)\n",
      "   Average clusters per timestep: 184.7\n",
      "   Average significant clusters per timestep: 6.8\n",
      "   Total significant volume: 18627.31 m¬≥\n",
      "   Total small volume: 25334.09 m¬≥\n",
      "   Volume fraction (significant): 42.4%\n",
      "\n",
      "   Top 10 largest individual clusters:\n",
      "      1. Timestep 12, Cluster 376: 700.49 m¬≥\n",
      "      2. Timestep 0, Cluster 1756: 370.46 m¬≥\n",
      "      3. Timestep 0, Cluster 1781: 315.91 m¬≥\n",
      "      4. Timestep 96, Cluster 36: 234.49 m¬≥\n",
      "      5. Timestep 2, Cluster 1575: 200.97 m¬≥\n",
      "      6. Timestep 2, Cluster 1288: 200.21 m¬≥\n",
      "      7. Timestep 30, Cluster 1453: 169.94 m¬≥\n",
      "      8. Timestep 97, Cluster 470: 154.89 m¬≥\n",
      "      9. Timestep 69, Cluster 679: 143.71 m¬≥\n",
      "      10. Timestep 11, Cluster 1030: 140.14 m¬≥\n",
      "\n",
      "============================================================\n",
      "Processing SanElijo - Volume Threshold: 10.0 m¬≥\n",
      "============================================================\n",
      "Loading data cubes...\n",
      "  Erosion cube shape: (115, 23825, 399)\n",
      "  Cluster cube shape: (115, 23825, 399)\n",
      "  Erosion value range: 0.00 to 17.97 cm\n",
      "  Cluster ID range: 1 to 1886\n",
      "Processing 115 timesteps...\n",
      "  Processed timestep 10/115 - Sig: 9, Small: 172\n",
      "  Processed timestep 20/115 - Sig: 26, Small: 380\n",
      "  Processed timestep 30/115 - Sig: 2, Small: 143\n",
      "  Processed timestep 40/115 - Sig: 2, Small: 143\n",
      "  Processed timestep 50/115 - Sig: 0, Small: 27\n",
      "  Processed timestep 60/115 - Sig: 0, Small: 39\n",
      "  Processed timestep 70/115 - Sig: 0, Small: 51\n",
      "  Processed timestep 80/115 - Sig: 0, Small: 19\n",
      "  Processed timestep 90/115 - Sig: 2, Small: 99\n",
      "  Processed timestep 100/115 - Sig: 0, Small: 26\n",
      "  Processed timestep 110/115 - Sig: 0, Small: 48\n",
      "  Processed timestep 115/115 - Sig: 2, Small: 40\n",
      "\n",
      "Cluster classification summary:\n",
      "  Total clusters across all timesteps: 11414\n",
      "  Significant clusters (>10.0 m¬≥): 314 (2.8%)\n",
      "  Small clusters (‚â§10.0 m¬≥): 11100 (97.2%)\n",
      "  Average clusters per timestep: 99.3\n",
      "\n",
      "Saving separated cubes...\n",
      "‚úÖ Saved significant erosion: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/cube_ero_10cm_sig.npz\n",
      "‚úÖ Saved small erosion: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/cube_ero_10cm_small.npz\n",
      "‚úÖ Saved significant clusters: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/cube_clusters_10cm_sig.npz\n",
      "‚úÖ Saved small clusters: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/cube_clusters_10cm_small.npz\n",
      "‚úÖ Saved cluster volumes: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/cluster_volumes_by_timestep.json\n",
      "‚úÖ Saved statistics: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/separation_stats.json\n",
      "\n",
      "üìä SanElijo Statistics:\n",
      "   Total timesteps: 115\n",
      "   Total clusters: 11414\n",
      "   Significant clusters: 314 (2.8%)\n",
      "   Small clusters: 11100 (97.2%)\n",
      "   Average clusters per timestep: 99.3\n",
      "   Average significant clusters per timestep: 2.7\n",
      "   Total significant volume: 8989.61 m¬≥\n",
      "   Total small volume: 12342.52 m¬≥\n",
      "   Volume fraction (significant): 42.1%\n",
      "\n",
      "   Top 10 largest individual clusters:\n",
      "      1. Timestep 25, Cluster 327: 551.30 m¬≥\n",
      "      2. Timestep 8, Cluster 77: 204.49 m¬≥\n",
      "      3. Timestep 25, Cluster 348: 189.65 m¬≥\n",
      "      4. Timestep 25, Cluster 357: 187.39 m¬≥\n",
      "      5. Timestep 8, Cluster 58: 160.95 m¬≥\n",
      "      6. Timestep 9, Cluster 426: 155.43 m¬≥\n",
      "      7. Timestep 19, Cluster 503: 137.44 m¬≥\n",
      "      8. Timestep 8, Cluster 64: 135.08 m¬≥\n",
      "      9. Timestep 19, Cluster 81: 129.99 m¬≥\n",
      "      10. Timestep 11, Cluster 385: 121.43 m¬≥\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "\n",
      "‚úÖ Successfully processed (5/5):\n",
      "   - Delmar\n",
      "   - Torrey\n",
      "   - Solana\n",
      "   - Encinitas\n",
      "   - SanElijo\n",
      "\n",
      "üéâ All 5 locations processed successfully!\n",
      "\n",
      "Output files for each location:\n",
      "   - cube_ero_10cm_sig.npz          (significant erosion events > 10.0 m¬≥)\n",
      "   - cube_ero_10cm_small.npz        (small erosion events ‚â§ 10.0 m¬≥)\n",
      "   - cube_clusters_10cm_sig.npz     (significant cluster IDs)\n",
      "   - cube_clusters_10cm_small.npz   (small cluster IDs)\n",
      "   - cluster_volumes_by_timestep.json (volumes for each cluster by timestep)\n",
      "   - separation_stats.json          (summary statistics)\n"
     ]
    }
   ],
   "source": [
    "def calculate_timestep_cluster_volumes(erosion_slice, cluster_slice, resolution_cm=10):\n",
    "    \"\"\"\n",
    "    Calculate volume for each cluster ID in a single timestep\n",
    "    \n",
    "    Args:\n",
    "        erosion_slice: (H, W) erosion values in cm for one timestep\n",
    "        cluster_slice: (H, W) cluster IDs for one timestep\n",
    "        resolution_cm: spatial resolution in cm\n",
    "    \n",
    "    Returns:\n",
    "        dict: {cluster_id: volume_m3} for this timestep only\n",
    "    \"\"\"\n",
    "    cluster_volumes = {}\n",
    "    \n",
    "    # Get unique cluster IDs (excluding 0/NaN)\n",
    "    unique_clusters = np.unique(cluster_slice)\n",
    "    unique_clusters = unique_clusters[unique_clusters > 0]  # Remove 0 and negative\n",
    "    unique_clusters = unique_clusters[~np.isnan(unique_clusters)]  # Remove NaN\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        # Get mask for this cluster\n",
    "        cluster_mask = cluster_slice == cluster_id\n",
    "        \n",
    "        # Get erosion values for this cluster (in cm)\n",
    "        cluster_erosion_cm = erosion_slice[cluster_mask]\n",
    "        \n",
    "        # Remove NaN values\n",
    "        cluster_erosion_cm = cluster_erosion_cm[~np.isnan(cluster_erosion_cm)]\n",
    "        \n",
    "        if len(cluster_erosion_cm) > 0:\n",
    "            # Calculate volume: sum(erosion_cm) * 0.01 (for 10cm pixels)\n",
    "            volume_m3 = np.sum(cluster_erosion_cm) * 0.01\n",
    "            cluster_volumes[cluster_id] = volume_m3\n",
    "    \n",
    "    return cluster_volumes\n",
    "\n",
    "\n",
    "def separate_erosion_by_volume(erosion_cube, cluster_cube, volume_threshold_m3=10.0, \n",
    "                                resolution_cm=10):\n",
    "    \"\"\"\n",
    "    Separate erosion cube into significant and small events based on cluster volume\n",
    "    Processing each timestep independently since cluster IDs reset each timestep\n",
    "    \n",
    "    Args:\n",
    "        erosion_cube: (T, H, W) erosion values\n",
    "        cluster_cube: (T, H, W) cluster IDs\n",
    "        volume_threshold_m3: threshold in cubic meters\n",
    "        resolution_cm: spatial resolution\n",
    "    \n",
    "    Returns:\n",
    "        erosion_sig: cube with only significant events\n",
    "        erosion_small: cube with only small events\n",
    "        cluster_sig: cube with cluster IDs for significant events only\n",
    "        cluster_small: cube with cluster IDs for small events only\n",
    "        all_cluster_volumes: list of dicts with volumes for each timestep\n",
    "        stats: dictionary with statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize output cubes\n",
    "    erosion_sig = np.full_like(erosion_cube, np.nan, dtype=float)\n",
    "    erosion_small = np.full_like(erosion_cube, np.nan, dtype=float)\n",
    "    cluster_sig = np.zeros_like(cluster_cube, dtype=cluster_cube.dtype)\n",
    "    cluster_small = np.zeros_like(cluster_cube, dtype=cluster_cube.dtype)\n",
    "    \n",
    "    # Store volumes for each timestep\n",
    "    all_cluster_volumes = []\n",
    "    \n",
    "    # Statistics tracking\n",
    "    total_clusters = 0\n",
    "    sig_clusters_count = 0\n",
    "    small_clusters_count = 0\n",
    "    sig_volume_total = 0.0\n",
    "    small_volume_total = 0.0\n",
    "    \n",
    "    print(f\"Processing {erosion_cube.shape[0]} timesteps...\")\n",
    "    \n",
    "    # Process each timestep independently\n",
    "    for t in range(erosion_cube.shape[0]):\n",
    "        # Calculate volumes for this timestep\n",
    "        timestep_volumes = calculate_timestep_cluster_volumes(\n",
    "            erosion_cube[t], cluster_cube[t], resolution_cm\n",
    "        )\n",
    "        \n",
    "        # Store volumes for this timestep\n",
    "        all_cluster_volumes.append(timestep_volumes)\n",
    "        \n",
    "        # Classify clusters for this timestep\n",
    "        sig_clusters = set([cid for cid, vol in timestep_volumes.items() if vol > volume_threshold_m3])\n",
    "        small_clusters = set([cid for cid, vol in timestep_volumes.items() if vol <= volume_threshold_m3])\n",
    "        \n",
    "        # Update statistics\n",
    "        total_clusters += len(timestep_volumes)\n",
    "        sig_clusters_count += len(sig_clusters)\n",
    "        small_clusters_count += len(small_clusters)\n",
    "        sig_volume_total += sum([vol for cid, vol in timestep_volumes.items() if cid in sig_clusters])\n",
    "        small_volume_total += sum([vol for cid, vol in timestep_volumes.items() if cid in small_clusters])\n",
    "        \n",
    "        # Create masks for this timestep\n",
    "        if sig_clusters:\n",
    "            sig_mask = np.isin(cluster_cube[t], list(sig_clusters))\n",
    "            erosion_sig[t][sig_mask] = erosion_cube[t][sig_mask]\n",
    "            cluster_sig[t][sig_mask] = cluster_cube[t][sig_mask]\n",
    "        \n",
    "        if small_clusters:\n",
    "            small_mask = np.isin(cluster_cube[t], list(small_clusters))\n",
    "            erosion_small[t][small_mask] = erosion_cube[t][small_mask]\n",
    "            cluster_small[t][small_mask] = cluster_cube[t][small_mask]\n",
    "        \n",
    "        if (t + 1) % 10 == 0 or t == erosion_cube.shape[0] - 1:\n",
    "            print(f\"  Processed timestep {t + 1}/{erosion_cube.shape[0]} - \"\n",
    "                  f\"Sig: {len(sig_clusters)}, Small: {len(small_clusters)}\")\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    stats = {\n",
    "        'total_timesteps': erosion_cube.shape[0],\n",
    "        'total_clusters': total_clusters,\n",
    "        'significant_clusters': sig_clusters_count,\n",
    "        'small_clusters': small_clusters_count,\n",
    "        'threshold_m3': volume_threshold_m3,\n",
    "        'significant_volume_m3': sig_volume_total,\n",
    "        'small_volume_m3': small_volume_total,\n",
    "        'significant_fraction': sig_clusters_count / total_clusters if total_clusters > 0 else 0,\n",
    "        'volume_fraction_significant': sig_volume_total / (sig_volume_total + small_volume_total) if (sig_volume_total + small_volume_total) > 0 else 0,\n",
    "        'avg_clusters_per_timestep': total_clusters / erosion_cube.shape[0] if erosion_cube.shape[0] > 0 else 0,\n",
    "        'avg_sig_clusters_per_timestep': sig_clusters_count / erosion_cube.shape[0] if erosion_cube.shape[0] > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCluster classification summary:\")\n",
    "    print(f\"  Total clusters across all timesteps: {total_clusters}\")\n",
    "    print(f\"  Significant clusters (>{volume_threshold_m3} m¬≥): {sig_clusters_count} ({stats['significant_fraction']*100:.1f}%)\")\n",
    "    print(f\"  Small clusters (‚â§{volume_threshold_m3} m¬≥): {small_clusters_count} ({(1-stats['significant_fraction'])*100:.1f}%)\")\n",
    "    print(f\"  Average clusters per timestep: {stats['avg_clusters_per_timestep']:.1f}\")\n",
    "    \n",
    "    return erosion_sig, erosion_small, cluster_sig, cluster_small, all_cluster_volumes, stats\n",
    "\n",
    "\n",
    "def process_location_separation(location, volume_threshold_m3=10.0, resolution_cm=10):\n",
    "    \"\"\"\n",
    "    Load existing data cubes, separate by volume, and save new files\n",
    "    \n",
    "    Args:\n",
    "        location: location name (e.g., \"Delmar\")\n",
    "        volume_threshold_m3: volume threshold in cubic meters\n",
    "        resolution_cm: spatial resolution in cm\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {location} - Volume Threshold: {volume_threshold_m3} m¬≥\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Define paths\n",
    "        data_dir = f\"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/{location}/data_cubes\"\n",
    "        \n",
    "        erosion_path = os.path.join(data_dir, \"cube_ero_10cm_filled.npz\")\n",
    "        cluster_path = os.path.join(data_dir, \"cube_clusters_ero_10cm_filled.npz\")\n",
    "        \n",
    "        # Check if files exist\n",
    "        if not os.path.exists(erosion_path):\n",
    "            print(f\"‚ùå Erosion cube not found: {erosion_path}\")\n",
    "            return False\n",
    "        \n",
    "        if not os.path.exists(cluster_path):\n",
    "            print(f\"‚ùå Cluster cube not found: {cluster_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"Loading data cubes...\")\n",
    "        erosion_cube = np.load(erosion_path)['data']\n",
    "        cluster_cube = np.load(cluster_path)['data']\n",
    "        \n",
    "        print(f\"  Erosion cube shape: {erosion_cube.shape}\")\n",
    "        print(f\"  Cluster cube shape: {cluster_cube.shape}\")\n",
    "        print(f\"  Erosion value range: {np.nanmin(erosion_cube):.2f} to {np.nanmax(erosion_cube):.2f} cm\")\n",
    "        print(f\"  Cluster ID range: {np.nanmin(cluster_cube[cluster_cube > 0]):.0f} to {np.nanmax(cluster_cube):.0f}\")\n",
    "        \n",
    "        # Separate by volume\n",
    "        erosion_sig, erosion_small, cluster_sig, cluster_small, all_cluster_volumes, stats = separate_erosion_by_volume(\n",
    "            erosion_cube, cluster_cube, volume_threshold_m3, resolution_cm\n",
    "        )\n",
    "        \n",
    "        # Save separated cubes\n",
    "        erosion_sig_path = os.path.join(data_dir, \"cube_ero_10cm_sig.npz\")\n",
    "        erosion_small_path = os.path.join(data_dir, \"cube_ero_10cm_small.npz\")\n",
    "        cluster_sig_path = os.path.join(data_dir, \"cube_clusters_10cm_sig.npz\")\n",
    "        cluster_small_path = os.path.join(data_dir, \"cube_clusters_10cm_small.npz\")\n",
    "        \n",
    "        print(f\"\\nSaving separated cubes...\")\n",
    "        np.savez_compressed(erosion_sig_path, data=erosion_sig)\n",
    "        np.savez_compressed(erosion_small_path, data=erosion_small)\n",
    "        np.savez_compressed(cluster_sig_path, data=cluster_sig)\n",
    "        np.savez_compressed(cluster_small_path, data=cluster_small)\n",
    "        \n",
    "        print(f\"‚úÖ Saved significant erosion: {erosion_sig_path}\")\n",
    "        print(f\"‚úÖ Saved small erosion: {erosion_small_path}\")\n",
    "        print(f\"‚úÖ Saved significant clusters: {cluster_sig_path}\")\n",
    "        print(f\"‚úÖ Saved small clusters: {cluster_small_path}\")\n",
    "        \n",
    "        # Save cluster volumes and statistics\n",
    "        volumes_path = os.path.join(data_dir, \"cluster_volumes_by_timestep.json\")\n",
    "        stats_path = os.path.join(data_dir, \"separation_stats.json\")\n",
    "        \n",
    "        # Convert numpy types to native Python types for JSON serialization\n",
    "        volumes_serializable = []\n",
    "        for t, timestep_vols in enumerate(all_cluster_volumes):\n",
    "            volumes_serializable.append({\n",
    "                'timestep': t,\n",
    "                'cluster_volumes': {int(k): float(v) for k, v in timestep_vols.items()}\n",
    "            })\n",
    "        \n",
    "        with open(volumes_path, \"w\") as f:\n",
    "            json.dump(volumes_serializable, f, indent=2)\n",
    "        \n",
    "        with open(stats_path, \"w\") as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Saved cluster volumes: {volumes_path}\")\n",
    "        print(f\"‚úÖ Saved statistics: {stats_path}\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nüìä {location} Statistics:\")\n",
    "        print(f\"   Total timesteps: {stats['total_timesteps']}\")\n",
    "        print(f\"   Total clusters: {stats['total_clusters']}\")\n",
    "        print(f\"   Significant clusters: {stats['significant_clusters']} ({stats['significant_fraction']*100:.1f}%)\")\n",
    "        print(f\"   Small clusters: {stats['small_clusters']} ({(1-stats['significant_fraction'])*100:.1f}%)\")\n",
    "        print(f\"   Average clusters per timestep: {stats['avg_clusters_per_timestep']:.1f}\")\n",
    "        print(f\"   Average significant clusters per timestep: {stats['avg_sig_clusters_per_timestep']:.1f}\")\n",
    "        print(f\"   Total significant volume: {stats['significant_volume_m3']:.2f} m¬≥\")\n",
    "        print(f\"   Total small volume: {stats['small_volume_m3']:.2f} m¬≥\")\n",
    "        print(f\"   Volume fraction (significant): {stats['volume_fraction_significant']*100:.1f}%\")\n",
    "        \n",
    "        # Find and print some examples of large clusters\n",
    "        large_examples = []\n",
    "        for t, timestep_vols in enumerate(all_cluster_volumes):\n",
    "            for cid, vol in timestep_vols.items():\n",
    "                if vol > volume_threshold_m3:\n",
    "                    large_examples.append((t, cid, vol))\n",
    "        \n",
    "        if large_examples:\n",
    "            large_examples.sort(key=lambda x: x[2], reverse=True)  # Sort by volume\n",
    "            print(f\"\\n   Top 10 largest individual clusters:\")\n",
    "            for i, (t, cid, vol) in enumerate(large_examples[:10]):\n",
    "                print(f\"      {i+1}. Timestep {t}, Cluster {int(cid)}: {vol:.2f} m¬≥\")\n",
    "        else:\n",
    "            print(f\"\\n   No individual clusters found above {volume_threshold_m3} m¬≥ threshold\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {location}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Process all locations and separate erosion by cluster volume\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    locations = [\"Delmar\", \"Torrey\", \"Solana\", \"Encinitas\", \"SanElijo\"] #, \"Blacks\"]\n",
    "    volume_threshold_m3 = 10.0  # 10 cubic meters\n",
    "    resolution_cm = 10  # 10 cm spatial resolution\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EROSION SEPARATION BY CLUSTER VOLUME (PER TIMESTEP)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Volume threshold: {volume_threshold_m3} m¬≥\")\n",
    "    print(f\"  Spatial resolution: {resolution_cm} cm\")\n",
    "    print(f\"  Locations to process: {len(locations)}\")\n",
    "    print(f\"  Processing: Each timestep independently (cluster IDs reset each timestep)\")\n",
    "    \n",
    "    # Process all locations\n",
    "    successful = []\n",
    "    failed = []\n",
    "    \n",
    "    for location in locations:\n",
    "        success = process_location_separation(location, volume_threshold_m3, resolution_cm)\n",
    "        \n",
    "        if success:\n",
    "            successful.append(location)\n",
    "        else:\n",
    "            failed.append(location)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully processed ({len(successful)}/{len(locations)}):\")\n",
    "    for loc in successful:\n",
    "        print(f\"   - {loc}\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\n‚ùå Failed to process ({len(failed)}/{len(locations)}):\")\n",
    "        for loc in failed:\n",
    "            print(f\"   - {loc}\")\n",
    "    else:\n",
    "        print(f\"\\nüéâ All {len(locations)} locations processed successfully!\")\n",
    "    \n",
    "    print(f\"\\nOutput files for each location:\")\n",
    "    print(f\"   - cube_ero_10cm_sig.npz          (significant erosion events > {volume_threshold_m3} m¬≥)\")\n",
    "    print(f\"   - cube_ero_10cm_small.npz        (small erosion events ‚â§ {volume_threshold_m3} m¬≥)\")\n",
    "    print(f\"   - cube_clusters_10cm_sig.npz     (significant cluster IDs)\")\n",
    "    print(f\"   - cube_clusters_10cm_small.npz   (small cluster IDs)\")\n",
    "    print(f\"   - cluster_volumes_by_timestep.json (volumes for each cluster by timestep)\")\n",
    "    print(f\"   - separation_stats.json          (summary statistics)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267df82",
   "metadata": {},
   "source": [
    "##### Plot by height to find logical cutoff elevation for large \"upper\" cliff vs \"lower\" cliff events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a0e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SIGNIFICANT EROSION EVENTS BUBBLE PLOT - FROM SIG CUBES ONLY\n",
      "============================================================\n",
      "Loading Delmar...\n",
      "  Loading significant erosion and cluster cubes...\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def get_cluster_elevation(cluster_slice, cluster_id):\n",
    "    \"\"\"\n",
    "    Calculate the mean elevation (centroid) of a cluster\n",
    "    \n",
    "    Args:\n",
    "        cluster_slice: (H, W) cluster IDs for one timestep  \n",
    "        cluster_id: the specific cluster ID to analyze\n",
    "    \n",
    "    Returns:\n",
    "        mean_elevation: mean column index (x-coordinate) of cluster pixels\n",
    "    \"\"\"\n",
    "    # Get mask for this cluster\n",
    "    cluster_mask = cluster_slice == cluster_id\n",
    "    \n",
    "    # Get row and column indices where cluster exists\n",
    "    row_indices, col_indices = np.where(cluster_mask)\n",
    "    \n",
    "    if len(col_indices) > 0:\n",
    "        # Return mean COLUMN index as elevation proxy\n",
    "        # (columns represent elevation/cross-shore distance)\n",
    "        mean_col = np.mean(col_indices)\n",
    "        return mean_col\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def load_sig_data_all_locations_with_elevation():\n",
    "    \"\"\"\n",
    "    Load significant erosion data from all locations and calculate actual elevations\n",
    "    \"\"\"\n",
    "    locations = [\"Delmar\", \"Torrey\", \"Solana\", \"Encinitas\", \"SanElijo\"]\n",
    "    location_colors = {\n",
    "        \"Delmar\": 0,\n",
    "        \"Torrey\": 1, \n",
    "        \"Solana\": 2,\n",
    "        \"Encinitas\": 3,\n",
    "        \"SanElijo\": 4\n",
    "    }\n",
    "    \n",
    "    all_events = []\n",
    "    \n",
    "    for location in locations:\n",
    "        print(f\"Loading {location}...\")\n",
    "        \n",
    "        try:\n",
    "            # Define paths\n",
    "            data_dir = f\"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/{location}/data_cubes\"\n",
    "            \n",
    "            # Load file list to get dates\n",
    "            files_path = os.path.join(data_dir, \"files_ero.json\")\n",
    "            volumes_path = os.path.join(data_dir, \"cluster_volumes_by_timestep.json\")\n",
    "            erosion_sig_path = os.path.join(data_dir, \"cube_ero_10cm_sig.npz\")\n",
    "            cluster_sig_path = os.path.join(data_dir, \"cube_clusters_10cm_sig.npz\")\n",
    "            \n",
    "            # Check if files exist\n",
    "            if not all(os.path.exists(p) for p in [files_path, volumes_path, erosion_sig_path, cluster_sig_path]):\n",
    "                print(f\"  ‚ö†Ô∏è  Missing files for {location}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Load data\n",
    "            with open(files_path, \"r\") as f:\n",
    "                file_list = json.load(f)\n",
    "            \n",
    "            with open(volumes_path, \"r\") as f:\n",
    "                volume_data = json.load(f)\n",
    "            \n",
    "            print(f\"  Loading significant erosion and cluster cubes...\")\n",
    "            erosion_sig_cube = np.load(erosion_sig_path)['data']\n",
    "            cluster_sig_cube = np.load(cluster_sig_path)['data']\n",
    "            \n",
    "            print(f\"  Found {len(file_list)} timesteps, sig cube shape: {erosion_sig_cube.shape}\")\n",
    "            \n",
    "            # Process each timestep\n",
    "            for timestep_data in volume_data:\n",
    "                t = timestep_data['timestep']\n",
    "                cluster_volumes = timestep_data['cluster_volumes']\n",
    "                \n",
    "                # Only include significant clusters (>10 m¬≥)\n",
    "                sig_clusters = {int(cid): vol for cid, vol in cluster_volumes.items() if vol > 10.0}\n",
    "                \n",
    "                if sig_clusters and t < erosion_sig_cube.shape[0]:\n",
    "                    # Get date from filename\n",
    "                    if t < len(file_list):\n",
    "                        filename = os.path.basename(file_list[t])\n",
    "                        \n",
    "                        # Extract date from filename\n",
    "                        import re\n",
    "                        date_match = re.search(r'(\\d{8})', filename)\n",
    "                        \n",
    "                        if date_match:\n",
    "                            date_str = date_match.group(1)\n",
    "                            try:\n",
    "                                date = datetime.strptime(date_str, '%Y%m%d')\n",
    "                                \n",
    "                                # Calculate elevation for each significant cluster\n",
    "                                for cluster_id, volume in sig_clusters.items():\n",
    "                                    # Check if this cluster actually exists in the significant cube\n",
    "                                    if np.any(cluster_sig_cube[t] == cluster_id):\n",
    "                                        elevation = get_cluster_elevation(\n",
    "                                            cluster_sig_cube[t], cluster_id\n",
    "                                        )\n",
    "                                        \n",
    "                                        if not np.isnan(elevation):\n",
    "                                            all_events.append({\n",
    "                                                'location': location,\n",
    "                                                'location_code': location_colors[location],\n",
    "                                                'date': date,\n",
    "                                                'timestep': t,\n",
    "                                                'cluster_id': cluster_id,\n",
    "                                                'volume': volume,\n",
    "                                                'elevation': elevation  # Actual elevation from sig cubes!\n",
    "                                            })\n",
    "                            except ValueError:\n",
    "                                print(f\"    Could not parse date from {filename}\")\n",
    "                \n",
    "            print(f\"  Found {len([e for e in all_events if e['location'] == location])} significant events\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error loading {location}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nTotal events across all locations: {len(all_events)}\")\n",
    "    return all_events\n",
    "\n",
    "def create_bubble_plot_with_real_elevation(events):\n",
    "    \"\"\"\n",
    "    Create bubble plot showing significant erosion events with real elevations\n",
    "    Individual subplots for each location + combined plot\n",
    "    \"\"\"\n",
    "    if not events:\n",
    "        print(\"No events to plot!\")\n",
    "        return\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    dates = [e['date'] for e in events]\n",
    "    elevations = [e['elevation'] for e in events]  # Now this is column indices\n",
    "    volumes = [e['volume'] for e in events]\n",
    "    locations_codes = [e['location_code'] for e in events]\n",
    "    location_names = [\"Delmar\", \"Torrey\", \"Solana\", \"Encinitas\", \"SanElijo\"]\n",
    "    \n",
    "    # Create figure with subplots: 3 rows, 2 columns\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Individual location plots (first 5 subplots)\n",
    "    for i, loc in enumerate(location_names):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Filter events for this location\n",
    "        loc_events = [e for e in events if e['location'] == loc]\n",
    "        \n",
    "        if loc_events:\n",
    "            loc_dates = [e['date'] for e in loc_events]\n",
    "            loc_elevations = [e['elevation'] for e in loc_events]\n",
    "            loc_volumes = [e['volume'] for e in loc_events]\n",
    "            \n",
    "            # Scale bubble sizes\n",
    "            bubble_sizes = [vol * 5 for vol in loc_volumes]\n",
    "            \n",
    "            # Create scatter plot colored by volume\n",
    "            scatter = ax.scatter(loc_dates, loc_elevations, s=bubble_sizes, c=loc_volumes, \n",
    "                               alpha=0.8, cmap='viridis', edgecolors='black', linewidth=0.5)\n",
    "            \n",
    "            # Add colorbar for volume\n",
    "            cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "            cbar.set_label('Volume (m¬≥)', fontsize=10)\n",
    "            \n",
    "            # Add horizontal line at appropriate column index for 5m elevation\n",
    "            ax.axhline(y=50, color='red', linestyle='--', alpha=0.7, linewidth=2, label='5m elevation')\n",
    "            \n",
    "            # Customize subplot\n",
    "            ax.set_title(f'{loc}\\n({len(loc_events)} events)', fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel('Date', fontsize=10)\n",
    "            ax.set_ylabel('Grid Column Index (Cross-shore Distance)', fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Format dates\n",
    "            import matplotlib.dates as mdates\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            ax.xaxis.set_major_locator(mdates.YearLocator(2))\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # Add legend for the horizontal line\n",
    "            ax.legend(fontsize=8)\n",
    "            \n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'No events\\nfor {loc}', \n",
    "                   transform=ax.transAxes, ha='center', va='center',\n",
    "                   fontsize=12, style='italic')\n",
    "            ax.set_title(f'{loc}\\n(0 events)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Combined plot (6th subplot, bottom right)\n",
    "    ax_combined = axes[5]\n",
    "    \n",
    "    # Scale bubble sizes for combined plot\n",
    "    bubble_sizes = [vol * 3 for vol in volumes]\n",
    "    \n",
    "    # Create scatter plot colored by location\n",
    "    scatter_combined = ax_combined.scatter(dates, elevations, s=bubble_sizes, c=locations_codes, \n",
    "                                         alpha=0.7, cmap='tab10', edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Add horizontal line at 5m elevation (column index)\n",
    "    ax_combined.axhline(y=50, color='red', linestyle='--', alpha=0.7, linewidth=2, label='5m elevation')\n",
    "    \n",
    "    # Customize combined plot\n",
    "    ax_combined.set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "    ax_combined.set_ylabel('Grid Column Index (Cross-shore Distance)', fontsize=12, fontweight='bold')\n",
    "    ax_combined.set_title(f'All Locations Combined\\n({len(events)} total events)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Format x-axis dates for combined plot\n",
    "    ax_combined.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax_combined.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax_combined.xaxis.set_minor_locator(mdates.MonthLocator([1, 7]))  # Jan and July\n",
    "    plt.setp(ax_combined.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Create location legend for combined plot\n",
    "    location_colors_rgb = plt.cm.tab10(np.linspace(0, 1, len(location_names)))\n",
    "    legend_elements = [mpatches.Patch(color=location_colors_rgb[i], label=loc) \n",
    "                      for i, loc in enumerate(location_names)]\n",
    "    \n",
    "    # Add legends to combined plot\n",
    "    legend1 = ax_combined.legend(handles=legend_elements, title='Location', \n",
    "                               loc='upper left', bbox_to_anchor=(1.02, 1), fontsize=9)\n",
    "    \n",
    "    # Create size legend for volumes\n",
    "    volume_ranges = [10, 50, 100, 500]\n",
    "    bubble_legend_sizes = [vol * 3 for vol in volume_ranges]\n",
    "    \n",
    "    legend_elements_size = []\n",
    "    for vol, size in zip(volume_ranges, bubble_legend_sizes):\n",
    "        legend_elements_size.append(plt.scatter([], [], s=size, c='gray', alpha=0.7, \n",
    "                                               edgecolors='black', linewidth=0.5))\n",
    "    \n",
    "    legend2 = ax_combined.legend(legend_elements_size, [f'{vol} m¬≥' for vol in volume_ranges],\n",
    "                               title='Volume', loc='upper left', bbox_to_anchor=(1.02, 0.6),\n",
    "                               scatterpoints=1, frameon=True, fontsize=9)\n",
    "    \n",
    "    # Add horizontal line legend\n",
    "    legend3 = ax_combined.legend(['5m elevation'], loc='upper left', bbox_to_anchor=(1.02, 0.3), fontsize=9)\n",
    "    \n",
    "    # Add all legends\n",
    "    ax_combined.add_artist(legend1)\n",
    "    ax_combined.add_artist(legend2)\n",
    "    \n",
    "    # Grid for combined plot\n",
    "    ax_combined.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle('Significant Erosion Events (>10 m¬≥) - Individual Locations + Combined\\nUsing Significant Cubes Only', \n",
    "                fontsize=16, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # Tight layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.91)  # Adjusted for 3x2 layout\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nüìä Plot Summary:\")\n",
    "    print(f\"   Total events plotted: {len(events)}\")\n",
    "    print(f\"   Date range: {min(dates).strftime('%Y-%m-%d')} to {max(dates).strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Volume range: {min(volumes):.1f} to {max(volumes):.1f} m¬≥\")\n",
    "    print(f\"   Elevation range: {min(elevations):.1f} to {max(elevations):.1f} (grid columns)\")\n",
    "    print(f\"   5m elevation marker at column: 50\")\n",
    "    \n",
    "    # Events by location\n",
    "    print(f\"\\n   Events by location:\")\n",
    "    for i, loc in enumerate(location_names):\n",
    "        count = len([e for e in events if e['location'] == loc])\n",
    "        if count > 0:\n",
    "            avg_elev = np.mean([e['elevation'] for e in events if e['location'] == loc])\n",
    "            avg_vol = np.mean([e['volume'] for e in events if e['location'] == loc])\n",
    "            above_5m = len([e for e in events if e['location'] == loc and e['elevation'] < 50])\n",
    "            below_5m = len([e for e in events if e['location'] == loc and e['elevation'] >= 50])\n",
    "            print(f\"     {loc}: {count} events, avg elevation: {avg_elev:.1f}, avg volume: {avg_vol:.1f} m¬≥\")\n",
    "            print(f\"              Above 5m: {above_5m}, Below 5m: {below_5m}\")\n",
    "        else:\n",
    "            print(f\"     {loc}: 0 events\")\n",
    "    \n",
    "    # Overall statistics about 5m threshold\n",
    "    total_above_5m = len([e for e in events if e['elevation'] < 50])\n",
    "    total_below_5m = len([e for e in events if e['elevation'] >= 50])\n",
    "    print(f\"\\n   Overall 5m elevation split:\")\n",
    "    print(f\"     Above 5m (cols 0-49): {total_above_5m} events ({total_above_5m/len(events)*100:.1f}%)\")\n",
    "    print(f\"     Below 5m (cols 50+): {total_below_5m} events ({total_below_5m/len(events)*100:.1f}%)\")\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data and create bubble plot with real elevations from significant cubes\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"SIGNIFICANT EROSION EVENTS BUBBLE PLOT - FROM SIG CUBES ONLY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load significant events from all locations with real elevations\n",
    "    events = load_sig_data_all_locations_with_elevation()\n",
    "    \n",
    "    if events:\n",
    "        # Create bubble plot with subplots\n",
    "        fig, axes = create_bubble_plot_with_real_elevation(events)\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig('significant_erosion_events_subplots.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n‚úÖ Plot saved as 'significant_erosion_events_subplots.png'\")\n",
    "        \n",
    "        # Show plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ùå No events found to plot!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08673ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coastal-cliff-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
