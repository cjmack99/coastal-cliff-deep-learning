{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25be397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.patches import Patch\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cb50a",
   "metadata": {},
   "source": [
    "#### Code to get gridded erosion data from cloud server and cluster into parallel data cubes of erosion metrics and cluster IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4c0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from reefbreak by location\n",
    "def find_csv_files(location, erosion=True, res_cm=10, cleaned=False):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of paths to all *_grid_{res_cm}cm.csv\n",
    "    in /â€¦/results/<location>/{erosion,deposition}/<date>/.\n",
    "    \"\"\"\n",
    "    base = \"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results\"\n",
    "    mode = \"erosion\" if erosion else \"deposition\"\n",
    "    root = os.path.join(base, location, mode)\n",
    "    \n",
    "    date_re = re.compile(r\"(\\d{8})\")\n",
    "    out = []\n",
    "    for date in sorted(os.listdir(root)):\n",
    "        ddir = os.path.join(root, date)\n",
    "        if not os.path.isdir(ddir):\n",
    "            continue\n",
    "        for fn in os.listdir(ddir):\n",
    "            if not cleaned:\n",
    "                if fn.endswith(f\"_grid_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "            else:\n",
    "                if fn.endswith(f\"_grid_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "    # sort by date prefix\n",
    "    out.sort(key=lambda x: x[0])\n",
    "    return [path for (_, path) in out]\n",
    "\n",
    "\n",
    "def find_cluster_csv_files(location, erosion=True, res_cm=10, cleaned=False):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of paths to all *_clusters_{res_cm}cm.csv files\n",
    "    in /â€¦/results/<location>/{erosion,deposition}/<date>/.\n",
    "    \"\"\"\n",
    "    base = \"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results\"\n",
    "    mode = \"erosion\" if erosion else \"deposition\"\n",
    "    root = os.path.join(base, location, mode)\n",
    "    \n",
    "    date_re = re.compile(r\"(\\d{8})\")\n",
    "    out = []\n",
    "    for date in sorted(os.listdir(root)):\n",
    "        ddir = os.path.join(root, date)\n",
    "        if not os.path.isdir(ddir):\n",
    "            continue\n",
    "        for fn in os.listdir(ddir):\n",
    "            if not cleaned:\n",
    "                # Look for cluster files instead of grid files\n",
    "                if fn.endswith(f\"_clusters_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "            else:\n",
    "                if fn.endswith(f\"_clusters_{res_cm}cm_filled.csv\") and (\n",
    "                (erosion and \"_ero_\" in fn) or (not erosion and \"_dep_\" in fn)\n",
    "                ):\n",
    "                    m = date_re.search(fn)\n",
    "                    if m:\n",
    "                        out.append((m.group(1), os.path.join(ddir, fn)))\n",
    "    # sort by date prefix\n",
    "    out.sort(key=lambda x: x[0])\n",
    "    return [path for (_, path) in out]\n",
    "\n",
    "# Load both grid and cluster data for a location\n",
    "def load_grid_and_cluster_cubes(location, erosion=True, res_cm=10, cleaned=False):\n",
    "    \"\"\"\n",
    "    Load both erosion/deposition grid data and cluster data for a location.\n",
    "    Returns grid_cube, grid_files, cluster_cube, cluster_files\n",
    "    \"\"\"\n",
    "    # Load grid data\n",
    "    grid_files = find_csv_files(location, erosion=erosion, res_cm=res_cm, cleaned=cleaned)\n",
    "    grid_cube, valid_grid_files = load_csv_to_numpy(grid_files)\n",
    "    \n",
    "    # Load cluster data\n",
    "    cluster_files = find_cluster_csv_files(location, erosion=erosion, res_cm=res_cm, cleaned=cleaned)\n",
    "    cluster_cube, valid_cluster_files = load_csv_to_numpy(cluster_files)\n",
    "    \n",
    "    print(f\"Grid cube shape: {grid_cube.shape}\")\n",
    "    print(f\"Cluster cube shape: {cluster_cube.shape}\")\n",
    "    \n",
    "    return grid_cube, valid_grid_files, cluster_cube, valid_cluster_files\n",
    "\n",
    "# turn csv grids into numpy arrays\n",
    "def load_csv_to_numpy(file_list):\n",
    "    \"\"\"\n",
    "    Loads CSV grids into a 3D NumPy array, with error checking for shape mismatches.\n",
    "    Mismatched grids are omitted from the final cube.\n",
    "    \"\"\"\n",
    "    grids = []\n",
    "    shapes = []\n",
    "    valid_files = []\n",
    "    \n",
    "    for fp in file_list:\n",
    "        df = pd.read_csv(fp, index_col=0)\n",
    "        grids.append(df.values)\n",
    "        shapes.append((fp, df.shape))\n",
    "        valid_files.append(fp)\n",
    "\n",
    "    if not grids:\n",
    "        raise ValueError(\"No valid CSV files found.\")\n",
    "    \n",
    "    # Get reference shape from first file\n",
    "    ref_shape = shapes[0][1]\n",
    "    \n",
    "    # Filter out mismatched grids\n",
    "    valid_grids = []\n",
    "    valid_files_final = []\n",
    "    mismatches = []\n",
    "    \n",
    "    for i, (fp, shape) in enumerate(shapes):\n",
    "        if shape == ref_shape:\n",
    "            valid_grids.append(grids[i])\n",
    "            valid_files_final.append(fp)\n",
    "        else:\n",
    "            mismatches.append((fp, shape))\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"\\nâš ï¸ Omitting {len(mismatches)} files with mismatched grid shapes:\")\n",
    "        for fp, shape in mismatches:\n",
    "            print(f\"  {os.path.basename(fp)} â€” shape = {shape}, expected = {ref_shape}\")\n",
    "        print(f\"\\nâœ… Using {len(valid_grids)} files with consistent shape {ref_shape}\")\n",
    "    \n",
    "    if not valid_grids:\n",
    "        raise ValueError(\"No files have consistent grid shapes.\")\n",
    "    \n",
    "    return np.stack(valid_grids, axis=0), valid_files_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db555bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing of all locations...\n",
      "Locations to process: Torrey, Solana, Encinitas, SanElijo, Blacks\n",
      "\n",
      "==================================================\n",
      "Processing Torrey...\n",
      "==================================================\n",
      "\n",
      "âš ï¸ Omitting 3 files with mismatched grid shapes:\n",
      "  20220518_to_20220518_ero_grid_10cm_filled.csv â€” shape = (12888, 1), expected = (12888, 750)\n",
      "  20230120_to_20230120_ero_grid_10cm_filled.csv â€” shape = (12888, 1), expected = (12888, 750)\n",
      "  20241126_to_20241126_ero_grid_10cm_filled.csv â€” shape = (12888, 1), expected = (12888, 750)\n",
      "\n",
      "âœ… Using 301 files with consistent shape (12888, 750)\n",
      "\n",
      "âš ï¸ Omitting 3 files with mismatched grid shapes:\n",
      "  20220518_to_20220518_ero_clusters_10cm_filled.csv â€” shape = (12888, 1), expected = (12888, 750)\n",
      "  20230120_to_20230120_ero_clusters_10cm_filled.csv â€” shape = (12888, 1), expected = (12888, 750)\n",
      "  20241126_to_20241126_ero_clusters_10cm_filled.csv â€” shape = (12888, 1), expected = (12888, 750)\n",
      "\n",
      "âœ… Using 301 files with consistent shape (12888, 750)\n",
      "Grid cube shape: (301, 12888, 750)\n",
      "Cluster cube shape: (301, 12888, 750)\n",
      "âœ… Saved grid cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/cube_ero_10cm_filled.npz\n",
      "âœ… Saved cluster cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/cube_clusters_ero_10cm_filled.npz\n",
      "âœ… Saved grid file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/files_ero.json\n",
      "âœ… Saved cluster file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/files_clusters_ero.json\n",
      "\n",
      "ðŸ“Š Torrey Summary:\n",
      "   Grid cube shape: (301, 12888, 750)\n",
      "   Cluster cube shape: (301, 12888, 750)\n",
      "   Number of time steps: 301\n",
      "   Grid file size: 21.68 GB\n",
      "   Cluster file size: 21.68 GB\n",
      "\n",
      "==================================================\n",
      "Processing Solana...\n",
      "==================================================\n",
      "Grid cube shape: (117, 28845, 500)\n",
      "Cluster cube shape: (117, 28845, 500)\n",
      "âœ… Saved grid cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/cube_ero_10cm_filled.npz\n",
      "âœ… Saved cluster cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/cube_clusters_ero_10cm_filled.npz\n",
      "âœ… Saved grid file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/files_ero.json\n",
      "âœ… Saved cluster file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/files_clusters_ero.json\n",
      "\n",
      "ðŸ“Š Solana Summary:\n",
      "   Grid cube shape: (117, 28845, 500)\n",
      "   Cluster cube shape: (117, 28845, 500)\n",
      "   Number of time steps: 117\n",
      "   Grid file size: 12.57 GB\n",
      "   Cluster file size: 12.57 GB\n",
      "\n",
      "==================================================\n",
      "Processing Encinitas...\n",
      "==================================================\n",
      "Grid cube shape: (104, 55824, 500)\n",
      "Cluster cube shape: (104, 55824, 500)\n",
      "âœ… Saved grid cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/cube_ero_10cm_filled.npz\n",
      "âœ… Saved cluster cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/cube_clusters_ero_10cm_filled.npz\n",
      "âœ… Saved grid file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/files_ero.json\n",
      "âœ… Saved cluster file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/files_clusters_ero.json\n",
      "\n",
      "ðŸ“Š Encinitas Summary:\n",
      "   Grid cube shape: (104, 55824, 500)\n",
      "   Cluster cube shape: (104, 55824, 500)\n",
      "   Number of time steps: 104\n",
      "   Grid file size: 21.63 GB\n",
      "   Cluster file size: 21.63 GB\n",
      "\n",
      "==================================================\n",
      "Processing SanElijo...\n",
      "==================================================\n",
      "Grid cube shape: (115, 23825, 399)\n",
      "Cluster cube shape: (115, 23825, 399)\n",
      "âœ… Saved grid cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/cube_ero_10cm_filled.npz\n",
      "âœ… Saved cluster cube: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/cube_clusters_ero_10cm_filled.npz\n",
      "âœ… Saved grid file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/files_ero.json\n",
      "âœ… Saved cluster file list: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/files_clusters_ero.json\n",
      "\n",
      "ðŸ“Š SanElijo Summary:\n",
      "   Grid cube shape: (115, 23825, 399)\n",
      "   Cluster cube shape: (115, 23825, 399)\n",
      "   Number of time steps: 115\n",
      "   Grid file size: 8.15 GB\n",
      "   Cluster file size: 8.15 GB\n",
      "\n",
      "==================================================\n",
      "Processing Blacks...\n",
      "==================================================\n",
      "âŒ Error processing Blacks: No valid CSV files found.\n",
      "\n",
      "============================================================\n",
      "BATCH PROCESSING COMPLETE\n",
      "============================================================\n",
      "\n",
      "âœ… Successfully processed (4/5):\n",
      "   - Torrey\n",
      "   - Solana\n",
      "   - Encinitas\n",
      "   - SanElijo\n",
      "\n",
      "âŒ Failed to process (1/5):\n",
      "   - Blacks\n",
      "\n",
      "Data cubes saved to:\n",
      "   Torrey: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Torrey/data_cubes/\n",
      "   Solana: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Solana/data_cubes/\n",
      "   Encinitas: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/Encinitas/data_cubes/\n",
      "   SanElijo: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/SanElijo/data_cubes/\n"
     ]
    }
   ],
   "source": [
    "# List of all locations to process\n",
    "locations = [\"Torrey\", \"Solana\", \"Encinitas\", \"SanElijo\", \"Blacks\"] # Delmar\", \n",
    "\n",
    "def process_location(location):\n",
    "    \"\"\"Process a single location: load data, create cubes, and save files\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {location}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        # Create data_cubes directory if it doesn't exist\n",
    "        output_dir = f\"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/{location}/data_cubes\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load both grid and cluster data for erosion\n",
    "        grid_cube, grid_files, cluster_cube, cluster_files = load_grid_and_cluster_cubes(\n",
    "            location, erosion=True, res_cm=10, cleaned=False\n",
    "        )\n",
    "        \n",
    "        # Save both cubes\n",
    "        grid_path = os.path.join(output_dir, \"cube_ero_10cm_filled.npz\")\n",
    "        cluster_path = os.path.join(output_dir, \"cube_clusters_ero_10cm_filled.npz\")\n",
    "        \n",
    "        np.savez_compressed(grid_path, data=grid_cube)\n",
    "        np.savez_compressed(cluster_path, data=cluster_cube)\n",
    "        \n",
    "        print(f\"âœ… Saved grid cube: {grid_path}\")\n",
    "        print(f\"âœ… Saved cluster cube: {cluster_path}\")\n",
    "        \n",
    "        # Save file lists\n",
    "        grid_files_path = os.path.join(output_dir, \"files_ero.json\")\n",
    "        cluster_files_path = os.path.join(output_dir, \"files_clusters_ero.json\")\n",
    "        \n",
    "        with open(grid_files_path, \"w\") as f:\n",
    "            json.dump(grid_files, f, indent=2)\n",
    "        \n",
    "        with open(cluster_files_path, \"w\") as f:\n",
    "            json.dump(cluster_files, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ… Saved grid file list: {grid_files_path}\")\n",
    "        print(f\"âœ… Saved cluster file list: {cluster_files_path}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nðŸ“Š {location} Summary:\")\n",
    "        print(f\"   Grid cube shape: {grid_cube.shape}\")\n",
    "        print(f\"   Cluster cube shape: {cluster_cube.shape}\")\n",
    "        print(f\"   Number of time steps: {len(grid_files)}\")\n",
    "        print(f\"   Grid file size: {grid_cube.nbytes / (1024**3):.2f} GB\")\n",
    "        print(f\"   Cluster file size: {cluster_cube.nbytes / (1024**3):.2f} GB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {location}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Process all locations\n",
    "successful_locations = []\n",
    "failed_locations = []\n",
    "\n",
    "print(\"Starting batch processing of all locations...\")\n",
    "print(f\"Locations to process: {', '.join(locations)}\")\n",
    "\n",
    "for location in locations:\n",
    "    success = process_location(location)\n",
    "    \n",
    "    if success:\n",
    "        successful_locations.append(location)\n",
    "    else:\n",
    "        failed_locations.append(location)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BATCH PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully processed ({len(successful_locations)}/{len(locations)}):\")\n",
    "for loc in successful_locations:\n",
    "    print(f\"   - {loc}\")\n",
    "\n",
    "if failed_locations:\n",
    "    print(f\"\\nâŒ Failed to process ({len(failed_locations)}/{len(locations)}):\")\n",
    "    for loc in failed_locations:\n",
    "        print(f\"   - {loc}\")\n",
    "else:\n",
    "    print(f\"\\nðŸŽ‰ All {len(locations)} locations processed successfully!\")\n",
    "\n",
    "print(f\"\\nData cubes saved to:\")\n",
    "for loc in successful_locations:\n",
    "    print(f\"   {loc}: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/{loc}/data_cubes/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca972ec",
   "metadata": {},
   "source": [
    "##### Code to split erosion cubes into small small and sig erosion events (10 cubic meter threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5503ff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EROSION SEPARATION BY CLUSTER VOLUME (PER TIMESTEP)\n",
      "============================================================\n",
      "\n",
      "Configuration:\n",
      "  Volume threshold: 10.0 mÂ³\n",
      "  Spatial resolution: 10 cm\n",
      "  Locations to process: 5\n",
      "  Processing: Each timestep independently (cluster IDs reset each timestep)\n",
      "\n",
      "============================================================\n",
      "Processing Delmar - Volume Threshold: 10.0 mÂ³\n",
      "============================================================\n",
      "Loading data cubes...\n"
     ]
    }
   ],
   "source": [
    "def calculate_timestep_cluster_volumes(erosion_slice, cluster_slice, resolution_cm=10):\n",
    "    \"\"\"\n",
    "    Calculate volume for each cluster ID in a single timestep\n",
    "    \n",
    "    Args:\n",
    "        erosion_slice: (H, W) erosion values in cm for one timestep\n",
    "        cluster_slice: (H, W) cluster IDs for one timestep\n",
    "        resolution_cm: spatial resolution in cm\n",
    "    \n",
    "    Returns:\n",
    "        dict: {cluster_id: volume_m3} for this timestep only\n",
    "    \"\"\"\n",
    "    cluster_volumes = {}\n",
    "    \n",
    "    # Get unique cluster IDs (excluding 0/NaN)\n",
    "    unique_clusters = np.unique(cluster_slice)\n",
    "    unique_clusters = unique_clusters[unique_clusters > 0]  # Remove 0 and negative\n",
    "    unique_clusters = unique_clusters[~np.isnan(unique_clusters)]  # Remove NaN\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        # Get mask for this cluster\n",
    "        cluster_mask = cluster_slice == cluster_id\n",
    "        \n",
    "        # Get erosion values for this cluster (in cm)\n",
    "        cluster_erosion_cm = erosion_slice[cluster_mask]\n",
    "        \n",
    "        # Remove NaN values\n",
    "        cluster_erosion_cm = cluster_erosion_cm[~np.isnan(cluster_erosion_cm)]\n",
    "        \n",
    "        if len(cluster_erosion_cm) > 0:\n",
    "            # Calculate volume: sum(erosion_cm) * 0.01 (for 10cm pixels)\n",
    "            volume_m3 = np.sum(cluster_erosion_cm) * 0.01\n",
    "            cluster_volumes[cluster_id] = volume_m3\n",
    "    \n",
    "    return cluster_volumes\n",
    "\n",
    "\n",
    "def separate_erosion_by_volume(erosion_cube, cluster_cube, volume_threshold_m3=10.0, \n",
    "                                resolution_cm=10):\n",
    "    \"\"\"\n",
    "    Separate erosion cube into significant and small events based on cluster volume\n",
    "    Processing each timestep independently since cluster IDs reset each timestep\n",
    "    \n",
    "    Args:\n",
    "        erosion_cube: (T, H, W) erosion values\n",
    "        cluster_cube: (T, H, W) cluster IDs\n",
    "        volume_threshold_m3: threshold in cubic meters\n",
    "        resolution_cm: spatial resolution\n",
    "    \n",
    "    Returns:\n",
    "        erosion_sig: cube with only significant events\n",
    "        erosion_small: cube with only small events\n",
    "        cluster_sig: cube with cluster IDs for significant events only\n",
    "        cluster_small: cube with cluster IDs for small events only\n",
    "        all_cluster_volumes: list of dicts with volumes for each timestep\n",
    "        stats: dictionary with statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize output cubes\n",
    "    erosion_sig = np.full_like(erosion_cube, np.nan, dtype=float)\n",
    "    erosion_small = np.full_like(erosion_cube, np.nan, dtype=float)\n",
    "    cluster_sig = np.zeros_like(cluster_cube, dtype=cluster_cube.dtype)\n",
    "    cluster_small = np.zeros_like(cluster_cube, dtype=cluster_cube.dtype)\n",
    "    \n",
    "    # Store volumes for each timestep\n",
    "    all_cluster_volumes = []\n",
    "    \n",
    "    # Statistics tracking\n",
    "    total_clusters = 0\n",
    "    sig_clusters_count = 0\n",
    "    small_clusters_count = 0\n",
    "    sig_volume_total = 0.0\n",
    "    small_volume_total = 0.0\n",
    "    \n",
    "    print(f\"Processing {erosion_cube.shape[0]} timesteps...\")\n",
    "    \n",
    "    # Process each timestep independently\n",
    "    for t in range(erosion_cube.shape[0]):\n",
    "        # Calculate volumes for this timestep\n",
    "        timestep_volumes = calculate_timestep_cluster_volumes(\n",
    "            erosion_cube[t], cluster_cube[t], resolution_cm\n",
    "        )\n",
    "        \n",
    "        # Store volumes for this timestep\n",
    "        all_cluster_volumes.append(timestep_volumes)\n",
    "        \n",
    "        # Classify clusters for this timestep\n",
    "        sig_clusters = set([cid for cid, vol in timestep_volumes.items() if vol > volume_threshold_m3])\n",
    "        small_clusters = set([cid for cid, vol in timestep_volumes.items() if vol <= volume_threshold_m3])\n",
    "        \n",
    "        # Update statistics\n",
    "        total_clusters += len(timestep_volumes)\n",
    "        sig_clusters_count += len(sig_clusters)\n",
    "        small_clusters_count += len(small_clusters)\n",
    "        sig_volume_total += sum([vol for cid, vol in timestep_volumes.items() if cid in sig_clusters])\n",
    "        small_volume_total += sum([vol for cid, vol in timestep_volumes.items() if cid in small_clusters])\n",
    "        \n",
    "        # Create masks for this timestep\n",
    "        if sig_clusters:\n",
    "            sig_mask = np.isin(cluster_cube[t], list(sig_clusters))\n",
    "            erosion_sig[t][sig_mask] = erosion_cube[t][sig_mask]\n",
    "            cluster_sig[t][sig_mask] = cluster_cube[t][sig_mask]\n",
    "        \n",
    "        if small_clusters:\n",
    "            small_mask = np.isin(cluster_cube[t], list(small_clusters))\n",
    "            erosion_small[t][small_mask] = erosion_cube[t][small_mask]\n",
    "            cluster_small[t][small_mask] = cluster_cube[t][small_mask]\n",
    "        \n",
    "        if (t + 1) % 10 == 0 or t == erosion_cube.shape[0] - 1:\n",
    "            print(f\"  Processed timestep {t + 1}/{erosion_cube.shape[0]} - \"\n",
    "                  f\"Sig: {len(sig_clusters)}, Small: {len(small_clusters)}\")\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    stats = {\n",
    "        'total_timesteps': erosion_cube.shape[0],\n",
    "        'total_clusters': total_clusters,\n",
    "        'significant_clusters': sig_clusters_count,\n",
    "        'small_clusters': small_clusters_count,\n",
    "        'threshold_m3': volume_threshold_m3,\n",
    "        'significant_volume_m3': sig_volume_total,\n",
    "        'small_volume_m3': small_volume_total,\n",
    "        'significant_fraction': sig_clusters_count / total_clusters if total_clusters > 0 else 0,\n",
    "        'volume_fraction_significant': sig_volume_total / (sig_volume_total + small_volume_total) if (sig_volume_total + small_volume_total) > 0 else 0,\n",
    "        'avg_clusters_per_timestep': total_clusters / erosion_cube.shape[0] if erosion_cube.shape[0] > 0 else 0,\n",
    "        'avg_sig_clusters_per_timestep': sig_clusters_count / erosion_cube.shape[0] if erosion_cube.shape[0] > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCluster classification summary:\")\n",
    "    print(f\"  Total clusters across all timesteps: {total_clusters}\")\n",
    "    print(f\"  Significant clusters (>{volume_threshold_m3} mÂ³): {sig_clusters_count} ({stats['significant_fraction']*100:.1f}%)\")\n",
    "    print(f\"  Small clusters (â‰¤{volume_threshold_m3} mÂ³): {small_clusters_count} ({(1-stats['significant_fraction'])*100:.1f}%)\")\n",
    "    print(f\"  Average clusters per timestep: {stats['avg_clusters_per_timestep']:.1f}\")\n",
    "    \n",
    "    return erosion_sig, erosion_small, cluster_sig, cluster_small, all_cluster_volumes, stats\n",
    "\n",
    "\n",
    "def process_location_separation(location, volume_threshold_m3=10.0, resolution_cm=10):\n",
    "    \"\"\"\n",
    "    Load existing data cubes, separate by volume, and save new files\n",
    "    \n",
    "    Args:\n",
    "        location: location name (e.g., \"Delmar\")\n",
    "        volume_threshold_m3: volume threshold in cubic meters\n",
    "        resolution_cm: spatial resolution in cm\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {location} - Volume Threshold: {volume_threshold_m3} mÂ³\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Define paths\n",
    "        data_dir = f\"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/{location}/data_cubes\"\n",
    "        \n",
    "        erosion_path = os.path.join(data_dir, \"cube_ero_10cm_filled.npz\")\n",
    "        cluster_path = os.path.join(data_dir, \"cube_clusters_ero_10cm_filled.npz\")\n",
    "        \n",
    "        # Check if files exist\n",
    "        if not os.path.exists(erosion_path):\n",
    "            print(f\"âŒ Erosion cube not found: {erosion_path}\")\n",
    "            return False\n",
    "        \n",
    "        if not os.path.exists(cluster_path):\n",
    "            print(f\"âŒ Cluster cube not found: {cluster_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"Loading data cubes...\")\n",
    "        erosion_cube = np.load(erosion_path)['data']\n",
    "        cluster_cube = np.load(cluster_path)['data']\n",
    "        \n",
    "        print(f\"  Erosion cube shape: {erosion_cube.shape}\")\n",
    "        print(f\"  Cluster cube shape: {cluster_cube.shape}\")\n",
    "        print(f\"  Erosion value range: {np.nanmin(erosion_cube):.2f} to {np.nanmax(erosion_cube):.2f} cm\")\n",
    "        print(f\"  Cluster ID range: {np.nanmin(cluster_cube[cluster_cube > 0]):.0f} to {np.nanmax(cluster_cube):.0f}\")\n",
    "        \n",
    "        # Separate by volume\n",
    "        erosion_sig, erosion_small, cluster_sig, cluster_small, all_cluster_volumes, stats = separate_erosion_by_volume(\n",
    "            erosion_cube, cluster_cube, volume_threshold_m3, resolution_cm\n",
    "        )\n",
    "        \n",
    "        # Save separated cubes\n",
    "        erosion_sig_path = os.path.join(data_dir, \"cube_ero_10cm_sig.npz\")\n",
    "        erosion_small_path = os.path.join(data_dir, \"cube_ero_10cm_small.npz\")\n",
    "        cluster_sig_path = os.path.join(data_dir, \"cube_clusters_10cm_sig.npz\")\n",
    "        cluster_small_path = os.path.join(data_dir, \"cube_clusters_10cm_small.npz\")\n",
    "        \n",
    "        print(f\"\\nSaving separated cubes...\")\n",
    "        np.savez_compressed(erosion_sig_path, data=erosion_sig)\n",
    "        np.savez_compressed(erosion_small_path, data=erosion_small)\n",
    "        np.savez_compressed(cluster_sig_path, data=cluster_sig)\n",
    "        np.savez_compressed(cluster_small_path, data=cluster_small)\n",
    "        \n",
    "        print(f\"âœ… Saved significant erosion: {erosion_sig_path}\")\n",
    "        print(f\"âœ… Saved small erosion: {erosion_small_path}\")\n",
    "        print(f\"âœ… Saved significant clusters: {cluster_sig_path}\")\n",
    "        print(f\"âœ… Saved small clusters: {cluster_small_path}\")\n",
    "        \n",
    "        # Save cluster volumes and statistics\n",
    "        volumes_path = os.path.join(data_dir, \"cluster_volumes_by_timestep.json\")\n",
    "        stats_path = os.path.join(data_dir, \"separation_stats.json\")\n",
    "        \n",
    "        # Convert numpy types to native Python types for JSON serialization\n",
    "        volumes_serializable = []\n",
    "        for t, timestep_vols in enumerate(all_cluster_volumes):\n",
    "            volumes_serializable.append({\n",
    "                'timestep': t,\n",
    "                'cluster_volumes': {int(k): float(v) for k, v in timestep_vols.items()}\n",
    "            })\n",
    "        \n",
    "        with open(volumes_path, \"w\") as f:\n",
    "            json.dump(volumes_serializable, f, indent=2)\n",
    "        \n",
    "        with open(stats_path, \"w\") as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ… Saved cluster volumes: {volumes_path}\")\n",
    "        print(f\"âœ… Saved statistics: {stats_path}\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nðŸ“Š {location} Statistics:\")\n",
    "        print(f\"   Total timesteps: {stats['total_timesteps']}\")\n",
    "        print(f\"   Total clusters: {stats['total_clusters']}\")\n",
    "        print(f\"   Significant clusters: {stats['significant_clusters']} ({stats['significant_fraction']*100:.1f}%)\")\n",
    "        print(f\"   Small clusters: {stats['small_clusters']} ({(1-stats['significant_fraction'])*100:.1f}%)\")\n",
    "        print(f\"   Average clusters per timestep: {stats['avg_clusters_per_timestep']:.1f}\")\n",
    "        print(f\"   Average significant clusters per timestep: {stats['avg_sig_clusters_per_timestep']:.1f}\")\n",
    "        print(f\"   Total significant volume: {stats['significant_volume_m3']:.2f} mÂ³\")\n",
    "        print(f\"   Total small volume: {stats['small_volume_m3']:.2f} mÂ³\")\n",
    "        print(f\"   Volume fraction (significant): {stats['volume_fraction_significant']*100:.1f}%\")\n",
    "        \n",
    "        # Find and print some examples of large clusters\n",
    "        large_examples = []\n",
    "        for t, timestep_vols in enumerate(all_cluster_volumes):\n",
    "            for cid, vol in timestep_vols.items():\n",
    "                if vol > volume_threshold_m3:\n",
    "                    large_examples.append((t, cid, vol))\n",
    "        \n",
    "        if large_examples:\n",
    "            large_examples.sort(key=lambda x: x[2], reverse=True)  # Sort by volume\n",
    "            print(f\"\\n   Top 10 largest individual clusters:\")\n",
    "            for i, (t, cid, vol) in enumerate(large_examples[:10]):\n",
    "                print(f\"      {i+1}. Timestep {t}, Cluster {int(cid)}: {vol:.2f} mÂ³\")\n",
    "        else:\n",
    "            print(f\"\\n   No individual clusters found above {volume_threshold_m3} mÂ³ threshold\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {location}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Process all locations and separate erosion by cluster volume\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    locations = [\"Delmar\", \"Torrey\", \"Solana\", \"Encinitas\", \"SanElijo\"] #, \"Blacks\"]\n",
    "    volume_threshold_m3 = 10.0  # 10 cubic meters\n",
    "    resolution_cm = 10  # 10 cm spatial resolution\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EROSION SEPARATION BY CLUSTER VOLUME (PER TIMESTEP)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Volume threshold: {volume_threshold_m3} mÂ³\")\n",
    "    print(f\"  Spatial resolution: {resolution_cm} cm\")\n",
    "    print(f\"  Locations to process: {len(locations)}\")\n",
    "    print(f\"  Processing: Each timestep independently (cluster IDs reset each timestep)\")\n",
    "    \n",
    "    # Process all locations\n",
    "    successful = []\n",
    "    failed = []\n",
    "    \n",
    "    for location in locations:\n",
    "        success = process_location_separation(location, volume_threshold_m3, resolution_cm)\n",
    "        \n",
    "        if success:\n",
    "            successful.append(location)\n",
    "        else:\n",
    "            failed.append(location)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Successfully processed ({len(successful)}/{len(locations)}):\")\n",
    "    for loc in successful:\n",
    "        print(f\"   - {loc}\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\nâŒ Failed to process ({len(failed)}/{len(locations)}):\")\n",
    "        for loc in failed:\n",
    "            print(f\"   - {loc}\")\n",
    "    else:\n",
    "        print(f\"\\nðŸŽ‰ All {len(locations)} locations processed successfully!\")\n",
    "    \n",
    "    print(f\"\\nOutput files for each location:\")\n",
    "    print(f\"   - cube_ero_10cm_sig.npz          (significant erosion events > {volume_threshold_m3} mÂ³)\")\n",
    "    print(f\"   - cube_ero_10cm_small.npz        (small erosion events â‰¤ {volume_threshold_m3} mÂ³)\")\n",
    "    print(f\"   - cube_clusters_10cm_sig.npz     (significant cluster IDs)\")\n",
    "    print(f\"   - cube_clusters_10cm_small.npz   (small cluster IDs)\")\n",
    "    print(f\"   - cluster_volumes_by_timestep.json (volumes for each cluster by timestep)\")\n",
    "    print(f\"   - separation_stats.json          (summary statistics)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a0e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08673ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coastal-cliff-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
