{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61daf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Slope Grids from LiDAR Point Clouds for Multiple Locations\n",
    "\n",
    "This script processes LAS files and creates gridded slope data that matches\n",
    "the dimensions of existing gridded elevation data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import laspy\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import re\n",
    "import platform\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84baea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LiDAR Vertical Slope Profile Generator\n",
      "======================================================================\n",
      "Platform: Darwin\n",
      "Base path: /Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs\n",
      "Vertical bin size: 0.1 m\n",
      "Moving window size: 3 bins\n",
      "Minimum points per bin: 5\n",
      "\n",
      "======================================================================\n",
      "Processing: Blacks - BlacksPolygones520to567at10cm\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: /opt/anaconda3/share/proj/proj.db contains DATABASE.LAYOUT.VERSION.MINOR = 2 whereas a number >= 6 is expected. It comes from another PROJ installation.\n"
     ]
    },
    {
     "ename": "CRSError",
     "evalue": "Invalid projection: EPSG:26911: (Internal Proj Error: proj_create: no database context specified)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCRSError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 508\u001b[39m\n\u001b[32m    504\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 500\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Process each location\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m location_name, polygon_name, start_mop, end_mop \u001b[38;5;129;01min\u001b[39;00m LOCATIONS:\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m     \u001b[43mprocess_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolygon_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m    503\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAll locations processed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 298\u001b[39m, in \u001b[36mprocess_location\u001b[39m\u001b[34m(location_name, polygon_name, start_mop, end_mop, overwrite)\u001b[39m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Load polygons (for visualization later)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m polys_gdf = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshp_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m polys_gdf[\u001b[33m\"\u001b[39m\u001b[33mPolygon_ID\u001b[39m\u001b[33m\"\u001b[39m] = polys_gdf.index\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Find all LAS files for this location\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/geopandas/io/file.py:316\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m             filename = response.read()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/geopandas/io/file.py:576\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     warnings.warn(\n\u001b[32m    568\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    572\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    573\u001b[39m     )\n\u001b[32m    574\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/pyogrio/geopandas.py:371\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m    369\u001b[39m geometry = shapely.from_wkb(geometry, on_invalid=on_invalid)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGeoDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcrs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/geopandas/geodataframe.py:243\u001b[39m, in \u001b[36mGeoDataFrame.__init__\u001b[39m\u001b[34m(self, data, geometry, crs, *args, **kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(geometry, pd.Series) \u001b[38;5;129;01mand\u001b[39;00m geometry.name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m    236\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    237\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    238\u001b[39m     ):\n\u001b[32m    239\u001b[39m         \u001b[38;5;66;03m# __init__ always creates geometry col named \"geometry\"\u001b[39;00m\n\u001b[32m    240\u001b[39m         \u001b[38;5;66;03m# rename as `set_geometry` respects the given series name\u001b[39;00m\n\u001b[32m    241\u001b[39m         geometry = geometry.rename(\u001b[33m\"\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_geometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m geometry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m crs:\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    247\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAssigning CRS to a GeoDataFrame without a geometry column is not \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupported. Supply geometry using the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mgeometry=\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword argument, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    249\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor by providing a DataFrame with column name \u001b[39m\u001b[33m'\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    250\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/geopandas/geodataframe.py:462\u001b[39m, in \u001b[36mGeoDataFrame.set_geometry\u001b[39m\u001b[34m(self, col, drop, inplace, crs)\u001b[39m\n\u001b[32m    459\u001b[39m     crs = \u001b[38;5;28mgetattr\u001b[39m(level, \u001b[33m\"\u001b[39m\u001b[33mcrs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    461\u001b[39m \u001b[38;5;66;03m# Check that we are using a listlike of geometries\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m level = \u001b[43m_ensure_geometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[38;5;66;03m# ensure_geometry only sets crs on level if it has crs==None\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level, GeoSeries):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/geopandas/geodataframe.py:71\u001b[39m, in \u001b[36m_ensure_geometry\u001b[39m\u001b[34m(data, crs)\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GeoSeries(out, index=data.index, name=data.name)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     out = \u001b[43mfrom_shapely\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/geopandas/array.py:190\u001b[39m, in \u001b[36mfrom_shapely\u001b[39m\u001b[34m(data, crs)\u001b[39m\n\u001b[32m    187\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput must be valid geometry objects: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeom\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    188\u001b[39m     arr = np.array(out, dtype=\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGeometryArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/geopandas/array.py:340\u001b[39m, in \u001b[36mGeometryArray.__init__\u001b[39m\u001b[34m(self, data, crs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28mself\u001b[39m._data = data\n\u001b[32m    339\u001b[39m \u001b[38;5;28mself\u001b[39m._crs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcrs\u001b[49m = crs\n\u001b[32m    341\u001b[39m \u001b[38;5;28mself\u001b[39m._sindex = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/geopandas/array.py:392\u001b[39m, in \u001b[36mGeometryArray.crs\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m HAS_PYPROJ:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyproj\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CRS\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m     \u001b[38;5;28mself\u001b[39m._crs = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mCRS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_user_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/pyproj/crs/crs.py:503\u001b[39m, in \u001b[36mCRS.from_user_input\u001b[39m\u001b[34m(cls, value, **kwargs)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mcls\u001b[39m):\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/pyproj/crs/crs.py:350\u001b[39m, in \u001b[36mCRS.__init__\u001b[39m\u001b[34m(self, projparams, **kwargs)\u001b[39m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m._local.crs = projparams\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[38;5;28mself\u001b[39m._local.crs = \u001b[43m_CRS\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msrs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/coastal-cliff-deep-learning/lib/python3.11/site-packages/pyproj/_crs.pyx:2364\u001b[39m, in \u001b[36mpyproj._crs._CRS.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCRSError\u001b[39m: Invalid projection: EPSG:26911: (Internal Proj Error: proj_create: no database context specified)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Create Vertical Slope Profiles from LiDAR Point Clouds\n",
    "\n",
    "This script processes LAS files and creates vertical slope profiles for each polygon.\n",
    "Each polygon represents an alongshore slice, and we compute slope vertically using\n",
    "elevation bins with a moving window approach.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Configuration\n",
    "LOCATIONS = [\n",
    "    ('Blacks', 'BlacksPolygones520to567at10cm', '520', '567'),\n",
    "    ('Blacks', 'BlacksPolygons520to567at1m', '520', '567'),\n",
    "    ('DelMar', 'DelMarPolygons595to620at1m', '595', '620'),\n",
    "    ('DelMar', 'DelMarPolygons595to620at10cm', '595', '620'),\n",
    "    ('Encinitas', 'EncinitasPolygones708to764at10cm', '708', '764'),\n",
    "    ('Encinitas', 'EncinitasPolygons708to764at1m', '708', '764'),\n",
    "    ('SanElijo', 'SanElijoPolygones684to708at10cm', '684', '708'),\n",
    "    ('SanElijo', 'SanElijoPolygons683to708at1m', '683', '708'),\n",
    "    ('Solana', 'SolanaPolygones637to666at10cm', '637', '666'),\n",
    "    ('Solana', 'SolanaPolygons637to666at1m', '637', '666'),\n",
    "    ('Torrey', 'TorreyPolygones568to581at10cm', '568', '581'),\n",
    "    ('Torrey', 'TorreyPolygons567to581at1m', '567', '581')\n",
    "]\n",
    "\n",
    "# Platform-specific base paths\n",
    "BASE = (\"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs\"\n",
    "        if platform.system() == \"Darwin\"\n",
    "        else \"/project/group/LiDAR/LidarProcessing/LidarProcessingCliffs\")\n",
    "\n",
    "BASE_SHAPE_PATH = Path(os.path.join(BASE, \"utilities\", \"shape_files\"))\n",
    "BASE_LAS_PATH = Path(os.path.join(BASE, \"results\"))\n",
    "OUTPUT_BASE_PATH = Path(os.path.join(BASE, \"results\", \"SlopeGrids\"))\n",
    "\n",
    "# Processing parameters\n",
    "VERTICAL_BIN_SIZE = 0.1  # 10 cm vertical bins\n",
    "MAX_HEIGHT = 30.0  # maximum height to consider\n",
    "WINDOW_SIZE = 3  # number of bins for moving window (centered)\n",
    "MIN_POINTS_PER_BIN = 5  # minimum points needed in a bin\n",
    "\n",
    "\n",
    "def compute_vertical_slope(z_centers, z_values, window_size=3):\n",
    "    \"\"\"\n",
    "    Compute slope at each vertical bin using a moving window.\n",
    "    \n",
    "    Args:\n",
    "        z_centers: Array of bin center elevations\n",
    "        z_values: Array of mean elevations in each bin\n",
    "        window_size: Number of bins to use in window (must be odd)\n",
    "    \n",
    "    Returns:\n",
    "        Array of slope values (same length as z_centers)\n",
    "    \"\"\"\n",
    "    n = len(z_centers)\n",
    "    slopes = np.full(n, np.nan)\n",
    "    \n",
    "    half_window = window_size // 2\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Get window indices\n",
    "        start_idx = max(0, i - half_window)\n",
    "        end_idx = min(n, i + half_window + 1)\n",
    "        \n",
    "        # Need at least 2 points to compute slope\n",
    "        if end_idx - start_idx < 2:\n",
    "            continue\n",
    "        \n",
    "        # Get window data\n",
    "        z_win = z_centers[start_idx:end_idx]\n",
    "        val_win = z_values[start_idx:end_idx]\n",
    "        \n",
    "        # Remove NaN values\n",
    "        valid = ~np.isnan(val_win)\n",
    "        if np.sum(valid) < 2:\n",
    "            continue\n",
    "        \n",
    "        z_win = z_win[valid]\n",
    "        val_win = val_win[valid]\n",
    "        \n",
    "        # Fit linear slope: dz/dh (change in position per change in height)\n",
    "        # Using polyfit for simple linear regression\n",
    "        if len(z_win) >= 2:\n",
    "            coeffs = np.polyfit(z_win, val_win, 1)\n",
    "            slopes[i] = abs(coeffs[0])  # slope magnitude\n",
    "    \n",
    "    return slopes\n",
    "\n",
    "\n",
    "def makeGrid_with_slope(pathin, pathout_slope, polys, \n",
    "                        vertical_bin_size=VERTICAL_BIN_SIZE,\n",
    "                        max_height=MAX_HEIGHT,\n",
    "                        window_size=WINDOW_SIZE,\n",
    "                        overwrite=False):\n",
    "    \"\"\"\n",
    "    Reads in a LAS file and a shapefile of polygons, calculates vertical slope profile\n",
    "    for each polygon using elevation bins with a moving window.\n",
    "    \n",
    "    Args:\n",
    "        pathin: Path to input LAS file\n",
    "        pathout_slope: Path to output CSV file for slope data\n",
    "        polys: Path to shapefile with polygons\n",
    "        vertical_bin_size: Size of vertical bins (default 0.1 m)\n",
    "        max_height: Maximum height to consider (default 30 m)\n",
    "        window_size: Number of bins for moving window (default 3)\n",
    "        overwrite: Whether to overwrite existing files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if output exists and skip if not overwriting\n",
    "    if Path(pathout_slope).exists() and not overwrite:\n",
    "        print(f\"Output exists, skipping: {pathout_slope}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n--- Processing LAS: {pathin} ---\")\n",
    "    \n",
    "    # Read LAS file\n",
    "    with laspy.open(pathin) as lasf:\n",
    "        las = lasf.read()\n",
    "    \n",
    "    print(f\"Loaded {len(las.x):,} points\")\n",
    "    \n",
    "    # Stack arrays (X, Y, Z)\n",
    "    arr = np.vstack((las.x, las.y, las.z)).T\n",
    "    \n",
    "    # Load polygons\n",
    "    polys_gdf = gpd.read_file(polys)\n",
    "    polys_gdf[\"Polygon_ID\"] = polys_gdf.index\n",
    "    print(f\"Loaded {len(polys_gdf)} polygons (alongshore slices)\")\n",
    "    \n",
    "    # Create points GeoDataFrame\n",
    "    df_pts = pd.DataFrame(arr, columns=['X', 'Y', 'Z'])\n",
    "    gdf_pts = gpd.GeoDataFrame(\n",
    "        df_pts,\n",
    "        geometry=[Point(x, y) for x, y in zip(arr[:, 0], arr[:, 1])],\n",
    "        crs=polys_gdf.crs\n",
    "    )\n",
    "    \n",
    "    # Spatial join to assign each point to a polygon\n",
    "    print(\"Performing spatial join...\")\n",
    "    joined = gpd.sjoin(gdf_pts, polys_gdf[['Polygon_ID', 'geometry']],\n",
    "                       how='inner', predicate='within')\n",
    "    \n",
    "    print(f\"{len(joined):,} points within polygons\")\n",
    "    \n",
    "    if len(joined) == 0:\n",
    "        print(\"No points within polygons, skipping...\")\n",
    "        return None\n",
    "    \n",
    "    # Create vertical bins\n",
    "    z_bin_edges = np.arange(0, max_height + vertical_bin_size, vertical_bin_size)\n",
    "    z_bin_centers = z_bin_edges[:-1] + vertical_bin_size / 2\n",
    "    \n",
    "    print(f\"Computing vertical slope profiles with {window_size}-bin moving window...\")\n",
    "    print(f\"Using {len(z_bin_centers)} vertical bins from 0 to {max_height} m\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each polygon\n",
    "    for poly_id in tqdm(range(len(polys_gdf)), desc=\"Processing polygons\"):\n",
    "        # Get points in this polygon\n",
    "        poly_points = joined[joined['Polygon_ID'] == poly_id]\n",
    "        \n",
    "        if len(poly_points) < MIN_POINTS_PER_BIN:\n",
    "            continue\n",
    "        \n",
    "        # Bin points by elevation\n",
    "        z_values = poly_points['Z'].values\n",
    "        x_values = poly_points['X'].values\n",
    "        y_values = poly_points['Y'].values\n",
    "        \n",
    "        # For each vertical bin, compute mean cross-shore position (X)\n",
    "        bin_indices = np.digitize(z_values, z_bin_edges) - 1\n",
    "        \n",
    "        # Calculate mean X position for each bin\n",
    "        mean_x_per_bin = np.full(len(z_bin_centers), np.nan)\n",
    "        points_per_bin = np.zeros(len(z_bin_centers), dtype=int)\n",
    "        \n",
    "        for i in range(len(z_bin_centers)):\n",
    "            mask = bin_indices == i\n",
    "            if np.sum(mask) >= MIN_POINTS_PER_BIN:\n",
    "                mean_x_per_bin[i] = np.mean(x_values[mask])\n",
    "                points_per_bin[i] = np.sum(mask)\n",
    "        \n",
    "        # Compute slope using moving window\n",
    "        slopes = compute_vertical_slope(z_bin_centers, mean_x_per_bin, window_size)\n",
    "        \n",
    "        # Store results for each bin with valid slope\n",
    "        for i, (z_center, slope, mean_x, n_pts) in enumerate(\n",
    "            zip(z_bin_centers, slopes, mean_x_per_bin, points_per_bin)\n",
    "        ):\n",
    "            if not np.isnan(slope) and n_pts >= MIN_POINTS_PER_BIN:\n",
    "                results.append({\n",
    "                    'Polygon_ID': poly_id,\n",
    "                    'z_bin': z_center,\n",
    "                    'slope_magnitude': slope,\n",
    "                    'slope_angle_deg': np.degrees(np.arctan(slope)),\n",
    "                    'mean_x': mean_x,\n",
    "                    'n_points': n_pts\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    if results:\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        Path(pathout_slope).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save detailed CSV\n",
    "        df_results.to_csv(pathout_slope, index=False)\n",
    "        print(f\"\\nSaved slope data to: {pathout_slope}\")\n",
    "        print(f\"Computed slopes for {len(df_results)} polygon-bin combinations\")\n",
    "        \n",
    "        # Also save as a grid (pivot table) to match your cube format\n",
    "        grid_file = pathout_slope.with_name(pathout_slope.stem + '_grid.csv')\n",
    "        slope_grid = df_results.pivot(index='Polygon_ID', columns='z_bin', values='slope_magnitude')\n",
    "        slope_grid.to_csv(grid_file)\n",
    "        print(f\"Saved gridded format to: {grid_file}\")\n",
    "        print(f\"Grid shape: {slope_grid.shape} (rows=polygons, cols=elevation bins)\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSlope Statistics:\")\n",
    "        print(f\"  Mean slope magnitude: {df_results['slope_magnitude'].mean():.3f}\")\n",
    "        print(f\"  Median slope magnitude: {df_results['slope_magnitude'].median():.3f}\")\n",
    "        print(f\"  Max slope magnitude: {df_results['slope_magnitude'].max():.3f}\")\n",
    "        print(f\"  Mean slope angle: {df_results['slope_angle_deg'].mean():.1f}°\")\n",
    "        \n",
    "        return df_results\n",
    "    else:\n",
    "        print(\"\\nNo valid slopes computed (insufficient points in bins)\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_slope_visualization(df_results, polys_gdf, output_file, location_name, date_str):\n",
    "    \"\"\"\n",
    "    Create visualization of vertical slope profiles.\n",
    "    \"\"\"\n",
    "    if df_results is None or len(df_results) == 0:\n",
    "        return\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Example vertical profiles for a few polygons\n",
    "    unique_polys = df_results['Polygon_ID'].unique()\n",
    "    n_profiles = min(10, len(unique_polys))\n",
    "    sample_polys = np.random.choice(unique_polys, n_profiles, replace=False)\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_profiles))\n",
    "    \n",
    "    for i, poly_id in enumerate(sample_polys):\n",
    "        poly_data = df_results[df_results['Polygon_ID'] == poly_id].sort_values('z_bin')\n",
    "        ax1.plot(poly_data['slope_magnitude'], poly_data['z_bin'], \n",
    "                'o-', color=colors[i], alpha=0.7, linewidth=2, markersize=4,\n",
    "                label=f'Polygon {poly_id}')\n",
    "    \n",
    "    ax1.set_xlabel('Slope Magnitude', fontsize=12)\n",
    "    ax1.set_ylabel('Elevation (m)', fontsize=12)\n",
    "    ax1.set_title(f'Example Vertical Slope Profiles\\n(n={n_profiles} polygons)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(fontsize=8, ncol=2)\n",
    "    \n",
    "    # Plot 2: Mean slope by elevation across all polygons\n",
    "    mean_slope_by_z = df_results.groupby('z_bin')['slope_magnitude'].agg(['mean', 'std', 'count'])\n",
    "    mean_slope_by_z = mean_slope_by_z[mean_slope_by_z['count'] >= 5]  # Only bins with enough data\n",
    "    \n",
    "    ax2.errorbar(mean_slope_by_z['mean'], mean_slope_by_z.index, \n",
    "                xerr=mean_slope_by_z['std'], fmt='o-', color='red', \n",
    "                linewidth=2, markersize=6, capsize=5, alpha=0.7)\n",
    "    ax2.set_xlabel('Mean Slope Magnitude', fontsize=12)\n",
    "    ax2.set_ylabel('Elevation (m)', fontsize=12)\n",
    "    ax2.set_title(f'Mean Slope Profile (All Polygons)\\n(n={len(unique_polys)} polygons)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    fig.suptitle(f'Vertical Slope Analysis\\n{location_name} - {date_str}',\n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved visualization to: {output_file}\")\n",
    "\n",
    "\n",
    "def process_location(location_name, polygon_name, start_mop, end_mop, overwrite=False):\n",
    "    \"\"\"Process all LAS files for a single location.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing: {location_name} - {polygon_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Construct shapefile path\n",
    "    shp_path = BASE_SHAPE_PATH / polygon_name / f\"{polygon_name}.shp\"\n",
    "    \n",
    "    if not shp_path.exists():\n",
    "        warnings.warn(f\"Shapefile not found: {shp_path}\\nSkipping...\")\n",
    "        return\n",
    "    \n",
    "    # Load polygons (for visualization later)\n",
    "    polys_gdf = gpd.read_file(shp_path)\n",
    "    polys_gdf[\"Polygon_ID\"] = polys_gdf.index\n",
    "    \n",
    "    # Find all LAS files for this location\n",
    "    las_folder = BASE_LAS_PATH / location_name / 'noveg'\n",
    "    \n",
    "    if not las_folder.exists():\n",
    "        warnings.warn(f\"LAS folder not found: {las_folder}\\nSkipping...\")\n",
    "        return\n",
    "    \n",
    "    las_files = sorted(las_folder.glob('*noveg.las'))\n",
    "    \n",
    "    if not las_files:\n",
    "        warnings.warn(f\"No LAS files found in: {las_folder}\\nSkipping...\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(las_files)} LAS files to process\")\n",
    "    \n",
    "    # Process each LAS file\n",
    "    for file_idx, las_file in enumerate(las_files, 1):\n",
    "        # Extract date from filename\n",
    "        date_match = re.match(r'^\\d{8}', las_file.stem)\n",
    "        date_str = date_match.group() if date_match else 'unknown'\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"File {file_idx}/{len(las_files)}: {las_file.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            # Create output paths\n",
    "            output_folder = OUTPUT_BASE_PATH / location_name / polygon_name\n",
    "            output_folder.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            pathout_slope = output_folder / f\"{date_str}_{polygon_name}_slopes.csv\"\n",
    "            fig_file = output_folder / f\"{date_str}_{polygon_name}_slopes.png\"\n",
    "            \n",
    "            # Process the file\n",
    "            df_results = makeGrid_with_slope(\n",
    "                pathin=las_file,\n",
    "                pathout_slope=pathout_slope,\n",
    "                polys=shp_path,\n",
    "                vertical_bin_size=VERTICAL_BIN_SIZE,\n",
    "                max_height=MAX_HEIGHT,\n",
    "                window_size=WINDOW_SIZE,\n",
    "                overwrite=overwrite\n",
    "            )\n",
    "            \n",
    "            # Create visualization if we got results\n",
    "            if df_results is not None and len(df_results) > 0:\n",
    "                create_slope_visualization(df_results, polys_gdf, fig_file, \n",
    "                                         location_name, date_str)\n",
    "        \n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error processing file {las_file.name}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nCompleted processing for {location_name} - {polygon_name}\")\n",
    "\n",
    "\n",
    "def load_slope_data(location, polygon_name, date_str, base_path=None, as_grid=False):\n",
    "    \"\"\"\n",
    "    Load slope data for a specific location, polygon, and date.\n",
    "    \n",
    "    Args:\n",
    "        location: Location name (e.g., 'DelMar')\n",
    "        polygon_name: Polygon identifier (e.g., 'DelMarPolygons595to620at10cm')\n",
    "        date_str: Date string (e.g., '20170323')\n",
    "        base_path: Base path to slope grids (optional)\n",
    "        as_grid: If True, load the grid format; if False, load detailed format\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with slope data (grid or detailed format)\n",
    "    \"\"\"\n",
    "    if base_path is None:\n",
    "        base_path = OUTPUT_BASE_PATH\n",
    "    else:\n",
    "        base_path = Path(base_path)\n",
    "    \n",
    "    if as_grid:\n",
    "        file_path = base_path / location / polygon_name / f\"{date_str}_{polygon_name}_slopes_grid.csv\"\n",
    "    else:\n",
    "        file_path = base_path / location / polygon_name / f\"{date_str}_{polygon_name}_slopes.csv\"\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Slope data file not found: {file_path}\")\n",
    "    \n",
    "    return pd.read_csv(file_path, index_col=0 if as_grid else None)\n",
    "\n",
    "\n",
    "def find_slope_grid_files(location, polygon_name, base_path=None):\n",
    "    \"\"\"\n",
    "    Find all slope grid files for a location, similar to find_csv_files().\n",
    "    Returns a sorted list of paths to all *_slopes_grid.csv files.\n",
    "    \n",
    "    Args:\n",
    "        location: Location name (e.g., 'DelMar')\n",
    "        polygon_name: Polygon identifier (e.g., 'DelMarPolygons595to620at10cm')\n",
    "        base_path: Base path to slope grids (optional)\n",
    "    \n",
    "    Returns:\n",
    "        list: Sorted list of file paths\n",
    "    \"\"\"\n",
    "    if base_path is None:\n",
    "        base_path = OUTPUT_BASE_PATH\n",
    "    else:\n",
    "        base_path = Path(base_path)\n",
    "    \n",
    "    folder = base_path / location / polygon_name\n",
    "    \n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    \n",
    "    # Find all grid files\n",
    "    grid_files = sorted(folder.glob(\"*_slopes_grid.csv\"))\n",
    "    \n",
    "    # Extract dates and sort\n",
    "    date_re = re.compile(r'(\\d{8})')\n",
    "    dated_files = []\n",
    "    for fp in grid_files:\n",
    "        m = date_re.search(fp.name)\n",
    "        if m:\n",
    "            dated_files.append((m.group(1), str(fp)))\n",
    "    \n",
    "    dated_files.sort(key=lambda x: x[0])\n",
    "    return [path for (_, path) in dated_files]\n",
    "\n",
    "\n",
    "def load_slope_cube(location, polygon_name, base_path=None):\n",
    "    \"\"\"\n",
    "    Load slope grids into a 3D NumPy array matching your cube format.\n",
    "    \n",
    "    Args:\n",
    "        location: Location name (e.g., 'DelMar')\n",
    "        polygon_name: Polygon identifier (e.g., 'DelMarPolygons595to620at10cm')\n",
    "        base_path: Base path to slope grids (optional)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (slope_cube, file_list) where slope_cube has shape (n_dates, n_polygons, n_z_bins)\n",
    "    \"\"\"\n",
    "    file_list = find_slope_grid_files(location, polygon_name, base_path)\n",
    "    \n",
    "    if not file_list:\n",
    "        raise ValueError(f\"No slope grid files found for {location}/{polygon_name}\")\n",
    "    \n",
    "    grids = []\n",
    "    valid_files = []\n",
    "    shapes = []\n",
    "    \n",
    "    for fp in file_list:\n",
    "        df = pd.read_csv(fp, index_col=0)\n",
    "        grids.append(df.values)\n",
    "        shapes.append((fp, df.shape))\n",
    "        valid_files.append(fp)\n",
    "    \n",
    "    if not grids:\n",
    "        raise ValueError(\"No valid slope grid files found.\")\n",
    "    \n",
    "    # Get reference shape\n",
    "    ref_shape = shapes[0][1]\n",
    "    \n",
    "    # Filter out mismatched grids\n",
    "    valid_grids = []\n",
    "    valid_files_final = []\n",
    "    mismatches = []\n",
    "    \n",
    "    for i, (fp, shape) in enumerate(shapes):\n",
    "        if shape == ref_shape:\n",
    "            valid_grids.append(grids[i])\n",
    "            valid_files_final.append(fp)\n",
    "        else:\n",
    "            mismatches.append((fp, shape))\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"\\n⚠️ Omitting {len(mismatches)} files with mismatched grid shapes:\")\n",
    "        for fp, shape in mismatches:\n",
    "            print(f\"  {os.path.basename(fp)} — shape = {shape}, expected = {ref_shape}\")\n",
    "        print(f\"\\n✅ Using {len(valid_grids)} files with consistent shape {ref_shape}\")\n",
    "    \n",
    "    if not valid_grids:\n",
    "        raise ValueError(\"No files have consistent grid shapes.\")\n",
    "    \n",
    "    slope_cube = np.stack(valid_grids, axis=0)\n",
    "    print(f\"Slope cube shape: {slope_cube.shape} (dates, polygons, elevation_bins)\")\n",
    "    \n",
    "    return slope_cube, valid_files_final\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main processing function.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"LiDAR Vertical Slope Profile Generator\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Platform: {platform.system()}\")\n",
    "    print(f\"Base path: {BASE}\")\n",
    "    print(f\"Vertical bin size: {VERTICAL_BIN_SIZE} m\")\n",
    "    print(f\"Moving window size: {WINDOW_SIZE} bins\")\n",
    "    print(f\"Minimum points per bin: {MIN_POINTS_PER_BIN}\")\n",
    "    \n",
    "    # Process each location\n",
    "    for location_name, polygon_name, start_mop, end_mop in LOCATIONS:\n",
    "        process_location(location_name, polygon_name, start_mop, end_mop, overwrite=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"All locations processed!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coastal-cliff-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
