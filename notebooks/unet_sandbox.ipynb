{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffFailureDataset(Dataset):\n",
    "    \"\"\"Dataset for cliff failure prediction using temporal sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, erosion_cube, cluster_cube, sequence_length=5, prediction_horizon=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            erosion_cube: (time, height, width) erosion rate data\n",
    "            cluster_cube: (time, height, width) cluster ID data  \n",
    "            sequence_length: number of time steps to use as input\n",
    "            prediction_horizon: how many steps ahead to predict failure\n",
    "        \"\"\"\n",
    "        self.erosion_cube = erosion_cube\n",
    "        self.cluster_cube = cluster_cube\n",
    "        self.seq_len = sequence_length\n",
    "        self.pred_horizon = prediction_horizon\n",
    "        \n",
    "        # Create failure labels by detecting large erosion events\n",
    "        self.failure_labels = self._create_failure_labels()\n",
    "        \n",
    "        # Generate valid indices for sequences\n",
    "        self.valid_indices = list(range(\n",
    "            sequence_length, \n",
    "            len(erosion_cube) - prediction_horizon\n",
    "        ))\n",
    "    \n",
    "    def _create_failure_labels(self):\n",
    "        \"\"\"Create binary failure labels based on erosion magnitude\"\"\"\n",
    "        # Define failure as erosion > 95th percentile in any location\n",
    "        threshold = np.nanpercentile(self.erosion_cube, 95)\n",
    "        failures = (self.erosion_cube > threshold).astype(float)\n",
    "        return failures\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get sequence of inputs and future failure label\"\"\"\n",
    "        time_idx = self.valid_indices[idx]\n",
    "        \n",
    "        # Input sequence: erosion + cluster data\n",
    "        erosion_seq = self.erosion_cube[time_idx-self.seq_len:time_idx]\n",
    "        cluster_seq = self.cluster_cube[time_idx-self.seq_len:time_idx]\n",
    "        \n",
    "        # Stack as 2-channel input (erosion, clusters)\n",
    "        input_seq = np.stack([erosion_seq, cluster_seq], axis=1)  # (seq_len, 2, H, W)\n",
    "        \n",
    "        # Target: failure probability map at future time\n",
    "        target = self.failure_labels[time_idx + self.pred_horizon]\n",
    "        \n",
    "        return torch.FloatTensor(input_seq), torch.FloatTensor(target)\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"ConvLSTM cell for processing spatial-temporal sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.input_dim + self.hidden_dim,\n",
    "            out_channels=4 * self.hidden_dim,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            bias=self.bias\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        \n",
    "        # Concatenate along channel axis\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        \n",
    "        # Split into gates\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        \n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "class UNetConvLSTM(nn.Module):\n",
    "    \"\"\"U-Net with ConvLSTM for cliff failure prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels=2, hidden_dims=[64, 128, 256], kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        # Encoder ConvLSTM layers\n",
    "        self.encoder_convlstms = nn.ModuleList()\n",
    "        self.encoder_convlstms.append(\n",
    "            ConvLSTMCell(input_channels, hidden_dims[0], kernel_size)\n",
    "        )\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            self.encoder_convlstms.append(\n",
    "                ConvLSTMCell(hidden_dims[i-1], hidden_dims[i], kernel_size)\n",
    "            )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_convs = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1, 0, -1):\n",
    "            self.decoder_convs.append(\n",
    "                nn.ConvTranspose2d(hidden_dims[i], hidden_dims[i-1], 4, 2, 1)\n",
    "            )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final_conv = nn.Conv2d(hidden_dims[0], 1, 1)\n",
    "        \n",
    "        # Pooling for encoder\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, channels, height, width)\n",
    "        Returns:\n",
    "            failure_prob: (batch, 1, height, width)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, channels, height, width = x.shape\n",
    "        \n",
    "        # Initialize hidden states\n",
    "        encoder_states = []\n",
    "        for i, hidden_dim in enumerate(self.hidden_dims):\n",
    "            h_size = (height // (2**i), width // (2**i))\n",
    "            h = torch.zeros(batch_size, hidden_dim, *h_size).to(x.device)\n",
    "            c = torch.zeros(batch_size, hidden_dim, *h_size).to(x.device)\n",
    "            encoder_states.append((h, c))\n",
    "        \n",
    "        # Process sequence through encoder\n",
    "        skip_connections = []\n",
    "        current_input = x\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            layer_input = current_input[:, t]  # (batch, channels, H, W)\n",
    "            layer_outputs = []\n",
    "            \n",
    "            for i, convlstm in enumerate(self.encoder_convlstms):\n",
    "                if i > 0:\n",
    "                    layer_input = self.pool(layer_outputs[i-1])\n",
    "                \n",
    "                h, c = convlstm(layer_input, encoder_states[i])\n",
    "                encoder_states[i] = (h, c)\n",
    "                layer_outputs.append(h)\n",
    "            \n",
    "            # Store skip connections from last timestep\n",
    "            if t == seq_len - 1:\n",
    "                skip_connections = layer_outputs[:-1]  # Exclude bottleneck\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = layer_outputs[-1]  # Bottleneck features\n",
    "        \n",
    "        for i, decoder_conv in enumerate(self.decoder_convs):\n",
    "            x = F.relu(decoder_conv(x))\n",
    "            if i < len(skip_connections):\n",
    "                # Add skip connection\n",
    "                skip_idx = len(skip_connections) - 1 - i\n",
    "                x = x + skip_connections[skip_idx]\n",
    "        \n",
    "        # Final prediction\n",
    "        failure_prob = torch.sigmoid(self.final_conv(x))\n",
    "        \n",
    "        return failure_prob\n",
    "\n",
    "def create_data_loaders(location, batch_size=8, test_size=0.2):\n",
    "    \"\"\"Create train/test data loaders from saved cubes\"\"\"\n",
    "    \n",
    "    # Load data cubes\n",
    "    base_path = f\"/Volumes/group/LiDAR/LidarProcessing/LidarProcessingCliffs/results/{location}/data_cubes\"\n",
    "    \n",
    "    erosion_data = np.load(f\"{base_path}/cube_ero_10cm_filled.npz\")['data']\n",
    "    cluster_data = np.load(f\"{base_path}/cube_clusters_ero_10cm_filled.npz\")['data']\n",
    "    \n",
    "    # Handle NaN values\n",
    "    erosion_data = np.nan_to_num(erosion_data, nan=0.0)\n",
    "    cluster_data = np.nan_to_num(cluster_data, nan=0.0)\n",
    "    \n",
    "    # Normalize erosion data\n",
    "    scaler = StandardScaler()\n",
    "    original_shape = erosion_data.shape\n",
    "    erosion_flat = erosion_data.reshape(-1, 1)\n",
    "    erosion_normalized = scaler.fit_transform(erosion_flat).reshape(original_shape)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = CliffFailureDataset(erosion_normalized, cluster_data)\n",
    "    \n",
    "    # Train/test split\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(dataset)), test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, scaler\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs=50, lr=0.001):\n",
    "    \"\"\"Train the U-Net ConvLSTM model\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            \n",
    "            # Reshape for loss computation\n",
    "            output = output.squeeze(1)  # Remove channel dim\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data).squeeze(1)\n",
    "                test_loss += criterion(output, target).item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        scheduler.step(avg_test_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "def predict_failure_risk(model, erosion_cube, cluster_cube, scaler, device='cpu'):\n",
    "    \"\"\"Generate failure risk predictions for entire time series\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Normalize erosion data\n",
    "    original_shape = erosion_cube.shape\n",
    "    erosion_flat = erosion_cube.reshape(-1, 1)\n",
    "    erosion_normalized = scaler.transform(erosion_flat).reshape(original_shape)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    erosion_normalized = np.nan_to_num(erosion_normalized, nan=0.0)\n",
    "    cluster_cube = np.nan_to_num(cluster_cube, nan=0.0)\n",
    "    \n",
    "    dataset = CliffFailureDataset(erosion_normalized, cluster_cube)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in dataloader:\n",
    "            data = data.to(device)\n",
    "            pred = model(data).squeeze().cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Usage example:\n",
    "def run_cliff_failure_analysis(location=\"Delmar\"):\n",
    "    \"\"\"Complete pipeline for cliff failure analysis\"\"\"\n",
    "    \n",
    "    print(f\"Setting up cliff failure analysis for {location}...\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, test_loader, scaler = create_data_loaders(location)\n",
    "    print(f\"Created datasets with {len(train_loader.dataset)} training samples\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = UNetConvLSTM(input_channels=2, hidden_dims=[32, 64, 128])\n",
    "    print(\"Initialized U-Net ConvLSTM model\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    train_losses, test_losses = train_model(model, train_loader, test_loader, num_epochs=30)\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Progress')\n",
    "    plt.savefig(f'{location}_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f'{location}_cliff_failure_model.pth')\n",
    "    print(f\"Model saved as {location}_cliff_failure_model.pth\")\n",
    "    \n",
    "    return model, scaler"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
